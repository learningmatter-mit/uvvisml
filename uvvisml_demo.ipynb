{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "uvvisml_demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/learningmatter-mit/uvvisml/blob/main/uvvisml_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jZnD-vU9Fd-"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in this notebook uses Chemprop v1.x, which supports `python>=3.7, <3.9`. When the notebook was first published, Google Colab used a Python version that matched these requirements. As of July 2025, Google Colab uses Python 3.11, and it is challenging to make Colab run a version of Python other than its default, or to change the Colab Python kernel. As a result, this notebook includes several \"hacks\" to allow the code to run in Python 3.8. Most of the setup steps and the `%%py38` cell magic at the beginning of each cell can be removed if this notebook is executed locally rather than in Colab."
      ],
      "metadata": {
        "id": "ctdL327X2WjL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjbRO4ZByzZD",
        "outputId": "efba90ff-4f05-4817-bf2c-8e15dd539705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-30 14:38:23--  https://repo.anaconda.com/miniconda/Miniconda3-py38_23.11.0-2-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.32.241, 104.16.191.158, 2606:4700::6810:bf9e, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.32.241|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 131844786 (126M) [application/octet-stream]\n",
            "Saving to: ‘miniconda.sh’\n",
            "\n",
            "miniconda.sh        100%[===================>] 125.74M   130MB/s    in 1.0s    \n",
            "\n",
            "2025-07-30 14:38:24 (130 MB/s) - ‘miniconda.sh’ saved [131844786/131844786]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "                                                                                    \n",
            "Installing base environment...\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Channels:\n",
            " - conda-forge\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - google-colab\n",
            "    - jupyter\n",
            "    - traitlets=5.5.0\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    aiofiles-22.1.0            |     pyhd8ed1ab_0          17 KB  conda-forge\n",
            "    aiohappyeyeballs-2.4.3     |     pyhd8ed1ab_0          19 KB  conda-forge\n",
            "    aiohttp-3.10.5             |   py38h2019614_0         697 KB  conda-forge\n",
            "    aiosignal-1.3.1            |     pyhd8ed1ab_0          12 KB  conda-forge\n",
            "    aiosqlite-0.20.0           |     pyhd8ed1ab_0          18 KB  conda-forge\n",
            "    anyio-3.7.1                |     pyhd8ed1ab_0          94 KB  conda-forge\n",
            "    archspec-0.2.3             |     pyhd8ed1ab_0          48 KB  conda-forge\n",
            "    argon2-cffi-23.1.0         |     pyhd8ed1ab_0          18 KB  conda-forge\n",
            "    argon2-cffi-bindings-21.2.0|   py38h01eb140_4          34 KB  conda-forge\n",
            "    arrow-1.3.0                |     pyhd8ed1ab_0          98 KB  conda-forge\n",
            "    asttokens-3.0.0            |     pyhd8ed1ab_0          27 KB  conda-forge\n",
            "    async-timeout-4.0.3        |     pyhd8ed1ab_0          11 KB  conda-forge\n",
            "    attrs-24.2.0               |     pyh71513ae_0          55 KB  conda-forge\n",
            "    babel-2.16.0               |     pyhd8ed1ab_0         6.2 MB  conda-forge\n",
            "    backcall-0.2.0             |     pyh9f0ad1d_0          13 KB  conda-forge\n",
            "    beautifulsoup4-4.12.3      |     pyha770c72_0         115 KB  conda-forge\n",
            "    bleach-6.1.0               |     pyhd8ed1ab_0         128 KB  conda-forge\n",
            "    ca-certificates-2025.7.14  |       hbd8a1cb_0         152 KB  conda-forge\n",
            "    cached-property-1.5.2      |       hd8ed1ab_1           4 KB  conda-forge\n",
            "    cached_property-1.5.2      |     pyha770c72_1          11 KB  conda-forge\n",
            "    cachetools-5.5.0           |     pyhd8ed1ab_0          14 KB  conda-forge\n",
            "    certifi-2024.8.30          |     pyhd8ed1ab_0         160 KB  conda-forge\n",
            "    comm-0.2.2                 |     pyhd8ed1ab_0          12 KB  conda-forge\n",
            "    conda-24.7.1               |   py38h578d9bd_0         930 KB  conda-forge\n",
            "    debugpy-1.6.7              |   py38h6a678d5_0         2.0 MB\n",
            "    decorator-5.1.1            |     pyhd8ed1ab_0          12 KB  conda-forge\n",
            "    defusedxml-0.7.1           |     pyhd8ed1ab_0          23 KB  conda-forge\n",
            "    entrypoints-0.4            |     pyhd8ed1ab_0           9 KB  conda-forge\n",
            "    exceptiongroup-1.2.2       |     pyhd8ed1ab_0          20 KB  conda-forge\n",
            "    executing-2.1.0            |     pyhd8ed1ab_0          28 KB  conda-forge\n",
            "    fqdn-1.5.1                 |     pyhd8ed1ab_0          14 KB  conda-forge\n",
            "    frozendict-2.4.4           |   py38hfb59056_0          48 KB  conda-forge\n",
            "    frozenlist-1.4.1           |   py38h01eb140_0          59 KB  conda-forge\n",
            "    google-auth-2.35.0         |     pyhff2d567_0         113 KB  conda-forge\n",
            "    google-colab-1.0.0         |     pyh44b312d_0          77 KB  conda-forge\n",
            "    importlib-metadata-8.5.0   |     pyha770c72_0          28 KB  conda-forge\n",
            "    importlib_metadata-8.5.0   |       hd8ed1ab_1           9 KB  conda-forge\n",
            "    importlib_resources-6.4.5  |     pyhd8ed1ab_0          32 KB  conda-forge\n",
            "    ipykernel-6.29.5           |     pyh3099207_0         116 KB  conda-forge\n",
            "    ipython-8.12.2             |     pyh41d4057_0         569 KB  conda-forge\n",
            "    ipython_genutils-0.2.0     |     pyhd8ed1ab_1          27 KB  conda-forge\n",
            "    ipywidgets-8.1.5           |     pyhd8ed1ab_0         111 KB  conda-forge\n",
            "    isoduration-20.11.0        |     pyhd8ed1ab_0          17 KB  conda-forge\n",
            "    jedi-0.19.1                |     pyhd8ed1ab_0         822 KB  conda-forge\n",
            "    jinja2-3.1.4               |     pyhd8ed1ab_0         109 KB  conda-forge\n",
            "    json5-0.9.25               |     pyhd8ed1ab_0          27 KB  conda-forge\n",
            "    jsonschema-4.23.0          |     pyhd8ed1ab_0          73 KB  conda-forge\n",
            "    jsonschema-specifications-2024.10.1|     pyhd8ed1ab_0          16 KB  conda-forge\n",
            "    jsonschema-with-format-nongpl-4.23.0|       hd8ed1ab_1           7 KB  conda-forge\n",
            "    jupyter-1.1.1              |     pyhd8ed1ab_0           9 KB  conda-forge\n",
            "    jupyter_client-7.4.9       |     pyhd8ed1ab_0          97 KB  conda-forge\n",
            "    jupyter_console-6.6.3      |     pyhd8ed1ab_0          26 KB  conda-forge\n",
            "    jupyter_core-5.8.1         |     pyh31011fe_0          58 KB  conda-forge\n",
            "    jupyter_events-0.10.0      |     pyhd8ed1ab_0          21 KB  conda-forge\n",
            "    jupyter_server-2.0.0       |     pyhd8ed1ab_0         294 KB  conda-forge\n",
            "    jupyter_server_fileid-0.9.2|     pyhd8ed1ab_0          20 KB  conda-forge\n",
            "    jupyter_server_terminals-0.5.3|     pyhd8ed1ab_0          19 KB  conda-forge\n",
            "    jupyter_server_ydoc-0.8.0  |     pyhd8ed1ab_0          15 KB  conda-forge\n",
            "    jupyter_ydoc-0.2.4         |     pyhd8ed1ab_0          78 KB  conda-forge\n",
            "    jupyterlab-3.6.8           |     pyhd8ed1ab_0         5.7 MB  conda-forge\n",
            "    jupyterlab_pygments-0.3.0  |     pyhd8ed1ab_0          18 KB  conda-forge\n",
            "    jupyterlab_server-2.27.3   |     pyhd8ed1ab_0          48 KB  conda-forge\n",
            "    jupyterlab_widgets-3.0.13  |     pyhd8ed1ab_0         182 KB  conda-forge\n",
            "    libblas-3.9.0              |32_h59b9bed_openblas          17 KB  conda-forge\n",
            "    libcblas-3.9.0             |32_he106b2a_openblas          17 KB  conda-forge\n",
            "    libgcc-15.1.0              |       h767d61c_3         806 KB  conda-forge\n",
            "    libgcc-ng-15.1.0           |       h69a702a_3          28 KB  conda-forge\n",
            "    libgfortran-15.1.0         |       h69a702a_3          28 KB  conda-forge\n",
            "    libgfortran5-15.1.0        |       hcea5267_3         1.5 MB  conda-forge\n",
            "    libgomp-15.1.0             |       h767d61c_3         437 KB  conda-forge\n",
            "    liblapack-3.9.0            |32_h7ac8fdf_openblas          17 KB  conda-forge\n",
            "    libopenblas-0.3.30         |pthreads_h94d23a6_1         5.6 MB  conda-forge\n",
            "    libsodium-1.0.18           |       h36c2ea0_1         366 KB  conda-forge\n",
            "    markupsafe-2.1.5           |   py38h01eb140_0          24 KB  conda-forge\n",
            "    matplotlib-inline-0.1.7    |     pyhd8ed1ab_0          14 KB  conda-forge\n",
            "    mistune-3.0.2              |     pyhd8ed1ab_0          64 KB  conda-forge\n",
            "    multidict-6.0.5            |   py38h01eb140_0          56 KB  conda-forge\n",
            "    nbclassic-1.1.0            |     pyhd8ed1ab_0         5.2 MB  conda-forge\n",
            "    nbclient-0.10.2            |     pyhd8ed1ab_0          27 KB  conda-forge\n",
            "    nbconvert-core-7.16.4      |     pyhff2d567_2         184 KB  conda-forge\n",
            "    nbformat-5.10.4            |     pyhd8ed1ab_0          99 KB  conda-forge\n",
            "    nest-asyncio-1.6.0         |     pyhd8ed1ab_0          11 KB  conda-forge\n",
            "    notebook-6.5.7             |     pyha770c72_0         301 KB  conda-forge\n",
            "    notebook-shim-0.2.4        |     pyhd8ed1ab_0          16 KB  conda-forge\n",
            "    numpy-1.22.3               |   py38h99721a1_2         6.8 MB  conda-forge\n",
            "    openssl-3.5.1              |       h7b32b05_0         3.0 MB  conda-forge\n",
            "    pandas-1.4.2               |   py38h47df419_1        12.6 MB  conda-forge\n",
            "    pandocfilters-1.5.0        |     pyhd8ed1ab_0          11 KB  conda-forge\n",
            "    parso-0.8.4                |     pyhd8ed1ab_0          73 KB  conda-forge\n",
            "    pexpect-4.9.0              |     pyhd8ed1ab_0          52 KB  conda-forge\n",
            "    pickleshare-0.7.5          |          py_1003           9 KB  conda-forge\n",
            "    pkgutil-resolve-name-1.3.10|     pyhd8ed1ab_1          11 KB  conda-forge\n",
            "    portpicker-1.6.0           |     pyhd8ed1ab_0          20 KB  conda-forge\n",
            "    prometheus_client-0.21.0   |     pyhd8ed1ab_0          48 KB  conda-forge\n",
            "    prompt-toolkit-3.0.48      |     pyha770c72_0         264 KB  conda-forge\n",
            "    prompt_toolkit-3.0.48      |       hd8ed1ab_1           6 KB  conda-forge\n",
            "    psutil-6.0.0               |   py38hfb59056_0         358 KB  conda-forge\n",
            "    ptyprocess-0.7.0           |     pyhd3deb0d_0          16 KB  conda-forge\n",
            "    pure_eval-0.2.3            |     pyhd8ed1ab_0          16 KB  conda-forge\n",
            "    pyasn1-0.6.1               |     pyhd8ed1ab_1          61 KB  conda-forge\n",
            "    pyasn1-modules-0.4.1       |     pyhd8ed1ab_0          94 KB  conda-forge\n",
            "    pygments-2.18.0            |     pyhd8ed1ab_0         859 KB  conda-forge\n",
            "    python-dateutil-2.9.0      |     pyhd8ed1ab_0         218 KB  conda-forge\n",
            "    python-fastjsonschema-2.20.0|     pyhd8ed1ab_0         221 KB  conda-forge\n",
            "    python-json-logger-2.0.7   |     pyhd8ed1ab_0          13 KB  conda-forge\n",
            "    python_abi-3.8             |           2_cp38           4 KB  conda-forge\n",
            "    pytz-2024.2                |     pyhd8ed1ab_0         183 KB  conda-forge\n",
            "    pyu2f-0.1.5                |     pyhd8ed1ab_0          31 KB  conda-forge\n",
            "    pyyaml-6.0.2               |   py38h2019614_0         181 KB  conda-forge\n",
            "    pyzmq-25.1.2               |   py38h6a678d5_0         463 KB\n",
            "    referencing-0.35.1         |     pyhd8ed1ab_0          41 KB  conda-forge\n",
            "    rfc3339-validator-0.1.4    |     pyhd8ed1ab_0           8 KB  conda-forge\n",
            "    rfc3986-validator-0.1.1    |     pyh9f0ad1d_0           8 KB  conda-forge\n",
            "    rpds-py-0.20.0             |   py38h4005ec7_0         327 KB  conda-forge\n",
            "    rsa-4.9                    |     pyhd8ed1ab_0          29 KB  conda-forge\n",
            "    send2trash-1.8.3           |     pyh0d859eb_0          22 KB  conda-forge\n",
            "    six-1.16.0                 |     pyh6c4a22f_0          14 KB  conda-forge\n",
            "    sniffio-1.3.1              |     pyhd8ed1ab_0          15 KB  conda-forge\n",
            "    soupsieve-2.6              |     pyhd8ed1ab_0          36 KB  conda-forge\n",
            "    stack_data-0.6.2           |     pyhd8ed1ab_0          26 KB  conda-forge\n",
            "    terminado-0.18.1           |     pyh0d859eb_0          22 KB  conda-forge\n",
            "    tinycss2-1.4.0             |     pyhd8ed1ab_0          28 KB  conda-forge\n",
            "    tomli-2.0.2                |     pyhd8ed1ab_0          18 KB  conda-forge\n",
            "    tornado-6.4.1              |   py38hfb59056_0         627 KB  conda-forge\n",
            "    traitlets-5.5.0            |     pyhd8ed1ab_0          85 KB  conda-forge\n",
            "    types-python-dateutil-2.9.0.20241003|     pyhff2d567_0          21 KB  conda-forge\n",
            "    typing-extensions-4.12.2   |       hd8ed1ab_0          10 KB  conda-forge\n",
            "    typing_extensions-4.12.2   |     pyha770c72_0          39 KB  conda-forge\n",
            "    uri-template-1.3.0         |     pyhd8ed1ab_0          23 KB  conda-forge\n",
            "    wcwidth-0.2.13             |     pyhd8ed1ab_0          32 KB  conda-forge\n",
            "    webcolors-24.8.0           |     pyhd8ed1ab_0          18 KB  conda-forge\n",
            "    webencodings-0.5.1         |     pyhd8ed1ab_2          15 KB  conda-forge\n",
            "    websocket-client-1.8.0     |     pyhd8ed1ab_0          46 KB  conda-forge\n",
            "    widgetsnbextension-4.0.13  |     pyhd8ed1ab_0         878 KB  conda-forge\n",
            "    y-py-0.5.9                 |   py38h9fda977_0         1.2 MB  conda-forge\n",
            "    yaml-0.2.5                 |       h280c20c_3          83 KB  conda-forge\n",
            "    yarl-1.9.4                 |   py38h01eb140_0         113 KB  conda-forge\n",
            "    ypy-websocket-0.8.2        |     pyhd8ed1ab_0          17 KB  conda-forge\n",
            "    zeromq-4.3.5               |       h6a678d5_0         366 KB\n",
            "    zipp-3.21.0                |     pyhd8ed1ab_0          21 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        64.2 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  aiofiles           conda-forge/noarch::aiofiles-22.1.0-pyhd8ed1ab_0 \n",
            "  aiohappyeyeballs   conda-forge/noarch::aiohappyeyeballs-2.4.3-pyhd8ed1ab_0 \n",
            "  aiohttp            conda-forge/linux-64::aiohttp-3.10.5-py38h2019614_0 \n",
            "  aiosignal          conda-forge/noarch::aiosignal-1.3.1-pyhd8ed1ab_0 \n",
            "  aiosqlite          conda-forge/noarch::aiosqlite-0.20.0-pyhd8ed1ab_0 \n",
            "  anyio              conda-forge/noarch::anyio-3.7.1-pyhd8ed1ab_0 \n",
            "  argon2-cffi        conda-forge/noarch::argon2-cffi-23.1.0-pyhd8ed1ab_0 \n",
            "  argon2-cffi-bindi~ conda-forge/linux-64::argon2-cffi-bindings-21.2.0-py38h01eb140_4 \n",
            "  arrow              conda-forge/noarch::arrow-1.3.0-pyhd8ed1ab_0 \n",
            "  asttokens          conda-forge/noarch::asttokens-3.0.0-pyhd8ed1ab_0 \n",
            "  async-timeout      conda-forge/noarch::async-timeout-4.0.3-pyhd8ed1ab_0 \n",
            "  attrs              conda-forge/noarch::attrs-24.2.0-pyh71513ae_0 \n",
            "  babel              conda-forge/noarch::babel-2.16.0-pyhd8ed1ab_0 \n",
            "  backcall           conda-forge/noarch::backcall-0.2.0-pyh9f0ad1d_0 \n",
            "  beautifulsoup4     conda-forge/noarch::beautifulsoup4-4.12.3-pyha770c72_0 \n",
            "  bleach             conda-forge/noarch::bleach-6.1.0-pyhd8ed1ab_0 \n",
            "  cached-property    conda-forge/noarch::cached-property-1.5.2-hd8ed1ab_1 \n",
            "  cached_property    conda-forge/noarch::cached_property-1.5.2-pyha770c72_1 \n",
            "  cachetools         conda-forge/noarch::cachetools-5.5.0-pyhd8ed1ab_0 \n",
            "  comm               conda-forge/noarch::comm-0.2.2-pyhd8ed1ab_0 \n",
            "  debugpy            pkgs/main/linux-64::debugpy-1.6.7-py38h6a678d5_0 \n",
            "  decorator          conda-forge/noarch::decorator-5.1.1-pyhd8ed1ab_0 \n",
            "  defusedxml         conda-forge/noarch::defusedxml-0.7.1-pyhd8ed1ab_0 \n",
            "  entrypoints        conda-forge/noarch::entrypoints-0.4-pyhd8ed1ab_0 \n",
            "  exceptiongroup     conda-forge/noarch::exceptiongroup-1.2.2-pyhd8ed1ab_0 \n",
            "  executing          conda-forge/noarch::executing-2.1.0-pyhd8ed1ab_0 \n",
            "  fqdn               conda-forge/noarch::fqdn-1.5.1-pyhd8ed1ab_0 \n",
            "  frozendict         conda-forge/linux-64::frozendict-2.4.4-py38hfb59056_0 \n",
            "  frozenlist         conda-forge/linux-64::frozenlist-1.4.1-py38h01eb140_0 \n",
            "  google-auth        conda-forge/noarch::google-auth-2.35.0-pyhff2d567_0 \n",
            "  google-colab       conda-forge/noarch::google-colab-1.0.0-pyh44b312d_0 \n",
            "  importlib-metadata conda-forge/noarch::importlib-metadata-8.5.0-pyha770c72_0 \n",
            "  importlib_metadata conda-forge/noarch::importlib_metadata-8.5.0-hd8ed1ab_1 \n",
            "  importlib_resourc~ conda-forge/noarch::importlib_resources-6.4.5-pyhd8ed1ab_0 \n",
            "  ipykernel          conda-forge/noarch::ipykernel-6.29.5-pyh3099207_0 \n",
            "  ipython            conda-forge/noarch::ipython-8.12.2-pyh41d4057_0 \n",
            "  ipython_genutils   conda-forge/noarch::ipython_genutils-0.2.0-pyhd8ed1ab_1 \n",
            "  ipywidgets         conda-forge/noarch::ipywidgets-8.1.5-pyhd8ed1ab_0 \n",
            "  isoduration        conda-forge/noarch::isoduration-20.11.0-pyhd8ed1ab_0 \n",
            "  jedi               conda-forge/noarch::jedi-0.19.1-pyhd8ed1ab_0 \n",
            "  jinja2             conda-forge/noarch::jinja2-3.1.4-pyhd8ed1ab_0 \n",
            "  json5              conda-forge/noarch::json5-0.9.25-pyhd8ed1ab_0 \n",
            "  jsonschema         conda-forge/noarch::jsonschema-4.23.0-pyhd8ed1ab_0 \n",
            "  jsonschema-specif~ conda-forge/noarch::jsonschema-specifications-2024.10.1-pyhd8ed1ab_0 \n",
            "  jsonschema-with-f~ conda-forge/noarch::jsonschema-with-format-nongpl-4.23.0-hd8ed1ab_1 \n",
            "  jupyter            conda-forge/noarch::jupyter-1.1.1-pyhd8ed1ab_0 \n",
            "  jupyter_client     conda-forge/noarch::jupyter_client-7.4.9-pyhd8ed1ab_0 \n",
            "  jupyter_console    conda-forge/noarch::jupyter_console-6.6.3-pyhd8ed1ab_0 \n",
            "  jupyter_core       conda-forge/noarch::jupyter_core-5.8.1-pyh31011fe_0 \n",
            "  jupyter_events     conda-forge/noarch::jupyter_events-0.10.0-pyhd8ed1ab_0 \n",
            "  jupyter_server     conda-forge/noarch::jupyter_server-2.0.0-pyhd8ed1ab_0 \n",
            "  jupyter_server_fi~ conda-forge/noarch::jupyter_server_fileid-0.9.2-pyhd8ed1ab_0 \n",
            "  jupyter_server_te~ conda-forge/noarch::jupyter_server_terminals-0.5.3-pyhd8ed1ab_0 \n",
            "  jupyter_server_yd~ conda-forge/noarch::jupyter_server_ydoc-0.8.0-pyhd8ed1ab_0 \n",
            "  jupyter_ydoc       conda-forge/noarch::jupyter_ydoc-0.2.4-pyhd8ed1ab_0 \n",
            "  jupyterlab         conda-forge/noarch::jupyterlab-3.6.8-pyhd8ed1ab_0 \n",
            "  jupyterlab_pygmen~ conda-forge/noarch::jupyterlab_pygments-0.3.0-pyhd8ed1ab_0 \n",
            "  jupyterlab_server  conda-forge/noarch::jupyterlab_server-2.27.3-pyhd8ed1ab_0 \n",
            "  jupyterlab_widgets conda-forge/noarch::jupyterlab_widgets-3.0.13-pyhd8ed1ab_0 \n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-32_h59b9bed_openblas \n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-32_he106b2a_openblas \n",
            "  libgcc             conda-forge/linux-64::libgcc-15.1.0-h767d61c_3 \n",
            "  libgfortran        conda-forge/linux-64::libgfortran-15.1.0-h69a702a_3 \n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-15.1.0-hcea5267_3 \n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-32_h7ac8fdf_openblas \n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.30-pthreads_h94d23a6_1 \n",
            "  libsodium          conda-forge/linux-64::libsodium-1.0.18-h36c2ea0_1 \n",
            "  markupsafe         conda-forge/linux-64::markupsafe-2.1.5-py38h01eb140_0 \n",
            "  matplotlib-inline  conda-forge/noarch::matplotlib-inline-0.1.7-pyhd8ed1ab_0 \n",
            "  mistune            conda-forge/noarch::mistune-3.0.2-pyhd8ed1ab_0 \n",
            "  multidict          conda-forge/linux-64::multidict-6.0.5-py38h01eb140_0 \n",
            "  nbclassic          conda-forge/noarch::nbclassic-1.1.0-pyhd8ed1ab_0 \n",
            "  nbclient           conda-forge/noarch::nbclient-0.10.2-pyhd8ed1ab_0 \n",
            "  nbconvert-core     conda-forge/noarch::nbconvert-core-7.16.4-pyhff2d567_2 \n",
            "  nbformat           conda-forge/noarch::nbformat-5.10.4-pyhd8ed1ab_0 \n",
            "  nest-asyncio       conda-forge/noarch::nest-asyncio-1.6.0-pyhd8ed1ab_0 \n",
            "  notebook           conda-forge/noarch::notebook-6.5.7-pyha770c72_0 \n",
            "  notebook-shim      conda-forge/noarch::notebook-shim-0.2.4-pyhd8ed1ab_0 \n",
            "  numpy              conda-forge/linux-64::numpy-1.22.3-py38h99721a1_2 \n",
            "  pandas             conda-forge/linux-64::pandas-1.4.2-py38h47df419_1 \n",
            "  pandocfilters      conda-forge/noarch::pandocfilters-1.5.0-pyhd8ed1ab_0 \n",
            "  parso              conda-forge/noarch::parso-0.8.4-pyhd8ed1ab_0 \n",
            "  pexpect            conda-forge/noarch::pexpect-4.9.0-pyhd8ed1ab_0 \n",
            "  pickleshare        conda-forge/noarch::pickleshare-0.7.5-py_1003 \n",
            "  pkgutil-resolve-n~ conda-forge/noarch::pkgutil-resolve-name-1.3.10-pyhd8ed1ab_1 \n",
            "  portpicker         conda-forge/noarch::portpicker-1.6.0-pyhd8ed1ab_0 \n",
            "  prometheus_client  conda-forge/noarch::prometheus_client-0.21.0-pyhd8ed1ab_0 \n",
            "  prompt-toolkit     conda-forge/noarch::prompt-toolkit-3.0.48-pyha770c72_0 \n",
            "  prompt_toolkit     conda-forge/noarch::prompt_toolkit-3.0.48-hd8ed1ab_1 \n",
            "  psutil             conda-forge/linux-64::psutil-6.0.0-py38hfb59056_0 \n",
            "  ptyprocess         conda-forge/noarch::ptyprocess-0.7.0-pyhd3deb0d_0 \n",
            "  pure_eval          conda-forge/noarch::pure_eval-0.2.3-pyhd8ed1ab_0 \n",
            "  pyasn1             conda-forge/noarch::pyasn1-0.6.1-pyhd8ed1ab_1 \n",
            "  pyasn1-modules     conda-forge/noarch::pyasn1-modules-0.4.1-pyhd8ed1ab_0 \n",
            "  pygments           conda-forge/noarch::pygments-2.18.0-pyhd8ed1ab_0 \n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.9.0-pyhd8ed1ab_0 \n",
            "  python-fastjsonsc~ conda-forge/noarch::python-fastjsonschema-2.20.0-pyhd8ed1ab_0 \n",
            "  python-json-logger conda-forge/noarch::python-json-logger-2.0.7-pyhd8ed1ab_0 \n",
            "  python_abi         conda-forge/linux-64::python_abi-3.8-2_cp38 \n",
            "  pytz               conda-forge/noarch::pytz-2024.2-pyhd8ed1ab_0 \n",
            "  pyu2f              conda-forge/noarch::pyu2f-0.1.5-pyhd8ed1ab_0 \n",
            "  pyyaml             conda-forge/linux-64::pyyaml-6.0.2-py38h2019614_0 \n",
            "  pyzmq              pkgs/main/linux-64::pyzmq-25.1.2-py38h6a678d5_0 \n",
            "  referencing        conda-forge/noarch::referencing-0.35.1-pyhd8ed1ab_0 \n",
            "  rfc3339-validator  conda-forge/noarch::rfc3339-validator-0.1.4-pyhd8ed1ab_0 \n",
            "  rfc3986-validator  conda-forge/noarch::rfc3986-validator-0.1.1-pyh9f0ad1d_0 \n",
            "  rpds-py            conda-forge/linux-64::rpds-py-0.20.0-py38h4005ec7_0 \n",
            "  rsa                conda-forge/noarch::rsa-4.9-pyhd8ed1ab_0 \n",
            "  send2trash         conda-forge/noarch::send2trash-1.8.3-pyh0d859eb_0 \n",
            "  six                conda-forge/noarch::six-1.16.0-pyh6c4a22f_0 \n",
            "  sniffio            conda-forge/noarch::sniffio-1.3.1-pyhd8ed1ab_0 \n",
            "  soupsieve          conda-forge/noarch::soupsieve-2.6-pyhd8ed1ab_0 \n",
            "  stack_data         conda-forge/noarch::stack_data-0.6.2-pyhd8ed1ab_0 \n",
            "  terminado          conda-forge/noarch::terminado-0.18.1-pyh0d859eb_0 \n",
            "  tinycss2           conda-forge/noarch::tinycss2-1.4.0-pyhd8ed1ab_0 \n",
            "  tomli              conda-forge/noarch::tomli-2.0.2-pyhd8ed1ab_0 \n",
            "  tornado            conda-forge/linux-64::tornado-6.4.1-py38hfb59056_0 \n",
            "  traitlets          conda-forge/noarch::traitlets-5.5.0-pyhd8ed1ab_0 \n",
            "  types-python-date~ conda-forge/noarch::types-python-dateutil-2.9.0.20241003-pyhff2d567_0 \n",
            "  typing-extensions  conda-forge/noarch::typing-extensions-4.12.2-hd8ed1ab_0 \n",
            "  typing_extensions  conda-forge/noarch::typing_extensions-4.12.2-pyha770c72_0 \n",
            "  uri-template       conda-forge/noarch::uri-template-1.3.0-pyhd8ed1ab_0 \n",
            "  wcwidth            conda-forge/noarch::wcwidth-0.2.13-pyhd8ed1ab_0 \n",
            "  webcolors          conda-forge/noarch::webcolors-24.8.0-pyhd8ed1ab_0 \n",
            "  webencodings       conda-forge/noarch::webencodings-0.5.1-pyhd8ed1ab_2 \n",
            "  websocket-client   conda-forge/noarch::websocket-client-1.8.0-pyhd8ed1ab_0 \n",
            "  widgetsnbextension conda-forge/noarch::widgetsnbextension-4.0.13-pyhd8ed1ab_0 \n",
            "  y-py               conda-forge/linux-64::y-py-0.5.9-py38h9fda977_0 \n",
            "  yaml               conda-forge/linux-64::yaml-0.2.5-h280c20c_3 \n",
            "  yarl               conda-forge/linux-64::yarl-1.9.4-py38h01eb140_0 \n",
            "  ypy-websocket      conda-forge/noarch::ypy-websocket-0.8.2-pyhd8ed1ab_0 \n",
            "  zeromq             pkgs/main/linux-64::zeromq-4.3.5-h6a678d5_0 \n",
            "  zipp               conda-forge/noarch::zipp-3.21.0-pyhd8ed1ab_0 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  archspec           pkgs/main::archspec-0.2.1-pyhd3eb1b0_0 --> conda-forge::archspec-0.2.3-pyhd8ed1ab_0 \n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2~ --> conda-forge/noarch::ca-certificates-2025.7.14-hbd8a1cb_0 \n",
            "  certifi            pkgs/main/linux-64::certifi-2023.11.1~ --> conda-forge/noarch::certifi-2024.8.30-pyhd8ed1ab_0 \n",
            "  conda              pkgs/main::conda-23.11.0-py38h06a4308~ --> conda-forge::conda-24.7.1-py38h578d9bd_0 \n",
            "  libgcc-ng          pkgs/main::libgcc-ng-11.2.0-h1234567_1 --> conda-forge::libgcc-ng-15.1.0-h69a702a_3 \n",
            "  libgomp              pkgs/main::libgomp-11.2.0-h1234567_1 --> conda-forge::libgomp-15.1.0-h767d61c_3 \n",
            "  openssl              pkgs/main::openssl-3.0.12-h7f8727e_0 --> conda-forge::openssl-3.5.1-h7b32b05_0 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Installed kernelspec py38 in /root/.local/share/jupyter/kernels/py38\n"
          ]
        }
      ],
      "source": [
        "# Install Python 3.8 (the maximum version supported by Chemprop 1.x) - Colab uses Python 3.11 as of July 2025\n",
        "# Copied from: https://raw.githubusercontent.com/j3soon/colab-python-version/refs/heads/main/scripts/py38.sh\n",
        "\n",
        "!wget -O miniconda.sh https://repo.anaconda.com/miniconda/Miniconda3-py38_23.11.0-2-Linux-x86_64.sh\n",
        "!bash ./miniconda.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter google-colab traitlets=5.5.0 -c conda-forge  # should take ~2 minutes\n",
        "!python -m ipykernel install --name \"py38\" --user\n",
        "!rm ./miniconda.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3qAcQGaQZii",
        "outputId": "c89c6fbf-9047-4fa6-fa37-639439d13d86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.18\n"
          ]
        }
      ],
      "source": [
        "!python --version # Python 3.8.18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9kSGiHeiYiN",
        "outputId": "fad2e527-1f7f-4522-c85f-17804a7133fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chemprop==1.7.1\n",
            "  Downloading chemprop-1.7.1-py3-none-any.whl.metadata (74 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting flask<=2.1.3,>=1.1.2 (from chemprop==1.7.1)\n",
            "  Downloading Flask-2.1.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting Werkzeug<3 (from chemprop==1.7.1)\n",
            "  Downloading werkzeug-2.3.8-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting hyperopt>=0.2.3 (from chemprop==1.7.1)\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting matplotlib>=3.1.3 (from chemprop==1.7.1)\n",
            "  Downloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.8/site-packages (from chemprop==1.7.1) (1.4.2)\n",
            "Collecting pandas-flavor>=0.2.0 (from chemprop==1.7.1)\n",
            "  Downloading pandas_flavor-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting scikit-learn>=0.22.2.post1 (from chemprop==1.7.1)\n",
            "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting sphinx>=3.1.2 (from chemprop==1.7.1)\n",
            "  Downloading sphinx-7.1.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting sphinx-rtd-theme>=2.0.0 (from chemprop==1.7.1)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting tensorboardX>=2.0 (from chemprop==1.7.1)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting torch>=1.4.0 (from chemprop==1.7.1)\n",
            "  Downloading torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: tqdm>=4.45.0 in /usr/local/lib/python3.8/site-packages (from chemprop==1.7.1) (4.65.0)\n",
            "Collecting typed-argument-parser>=1.6.1 (from chemprop==1.7.1)\n",
            "  Downloading typed_argument_parser-1.10.1-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting rdkit>=2020.03.1.0 (from chemprop==1.7.1)\n",
            "  Downloading rdkit-2024.3.5-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting scipy>=1.9 (from chemprop==1.7.1)\n",
            "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting descriptastorus>=2.6.1 (from chemprop==1.7.1)\n",
            "  Downloading descriptastorus-2.8.0-py3-none-any.whl.metadata (364 bytes)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.8/site-packages (from flask<=2.1.3,>=1.1.2->chemprop==1.7.1) (3.1.4)\n",
            "Collecting itsdangerous>=2.0 (from flask<=2.1.3,>=1.1.2->chemprop==1.7.1)\n",
            "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting click>=8.0 (from flask<=2.1.3,>=1.1.2->chemprop==1.7.1)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.8/site-packages (from flask<=2.1.3,>=1.1.2->chemprop==1.7.1) (8.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/site-packages (from hyperopt>=0.2.3->chemprop==1.7.1) (1.16.0)\n",
            "Collecting networkx>=2.2 (from hyperopt>=0.2.3->chemprop==1.7.1)\n",
            "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting future (from hyperopt>=0.2.3->chemprop==1.7.1)\n",
            "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting cloudpickle (from hyperopt>=0.2.3->chemprop==1.7.1)\n",
            "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting py4j (from hyperopt>=0.2.3->chemprop==1.7.1)\n",
            "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib>=3.1.3->chemprop==1.7.1)\n",
            "  Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib>=3.1.3->chemprop==1.7.1)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib>=3.1.3->chemprop==1.7.1)\n",
            "  Downloading fonttools-4.57.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.3/102.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib>=3.1.3->chemprop==1.7.1)\n",
            "  Downloading kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from matplotlib>=3.1.3->chemprop==1.7.1) (23.1)\n",
            "Collecting pillow>=6.2.0 (from matplotlib>=3.1.3->chemprop==1.7.1)\n",
            "  Downloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib>=3.1.3->chemprop==1.7.1)\n",
            "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/site-packages (from matplotlib>=3.1.3->chemprop==1.7.1) (2.9.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/site-packages (from matplotlib>=3.1.3->chemprop==1.7.1) (6.4.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/site-packages (from pandas>=1.0.3->chemprop==1.7.1) (2024.2)\n",
            "Collecting xarray (from pandas-flavor>=0.2.0->chemprop==1.7.1)\n",
            "  Downloading xarray-2023.1.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting joblib>=1.1.1 (from scikit-learn>=0.22.2.post1->chemprop==1.7.1)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.22.2.post1->chemprop==1.7.1)\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting sphinxcontrib-applehelp (from sphinx>=3.1.2->chemprop==1.7.1)\n",
            "  Downloading sphinxcontrib_applehelp-1.0.4-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting sphinxcontrib-devhelp (from sphinx>=3.1.2->chemprop==1.7.1)\n",
            "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting sphinxcontrib-jsmath (from sphinx>=3.1.2->chemprop==1.7.1)\n",
            "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx>=3.1.2->chemprop==1.7.1)\n",
            "  Downloading sphinxcontrib_htmlhelp-2.0.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting sphinxcontrib-serializinghtml>=1.1.5 (from sphinx>=3.1.2->chemprop==1.7.1)\n",
            "  Downloading sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting sphinxcontrib-qthelp (from sphinx>=3.1.2->chemprop==1.7.1)\n",
            "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: Pygments>=2.13 in /usr/local/lib/python3.8/site-packages (from sphinx>=3.1.2->chemprop==1.7.1) (2.18.0)\n",
            "Collecting docutils<0.21,>=0.18.1 (from sphinx>=3.1.2->chemprop==1.7.1)\n",
            "  Downloading docutils-0.20.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting snowballstemmer>=2.0 (from sphinx>=3.1.2->chemprop==1.7.1)\n",
            "  Downloading snowballstemmer-3.0.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: babel>=2.9 in /usr/local/lib/python3.8/site-packages (from sphinx>=3.1.2->chemprop==1.7.1) (2.16.0)\n",
            "Collecting alabaster<0.8,>=0.7 (from sphinx>=3.1.2->chemprop==1.7.1)\n",
            "  Downloading alabaster-0.7.13-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting imagesize>=1.3 (from sphinx>=3.1.2->chemprop==1.7.1)\n",
            "  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.8/site-packages (from sphinx>=3.1.2->chemprop==1.7.1) (2.31.0)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme>=2.0.0->chemprop==1.7.1)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting protobuf>=3.20 (from tensorboardX>=2.0->chemprop==1.7.1)\n",
            "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting filelock (from torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.8/site-packages (from torch>=1.4.0->chemprop==1.7.1) (4.12.2)\n",
            "Collecting sympy (from torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting fsspec (from torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting docstring-parser>=0.15 (from typed-argument-parser>=1.6.1->chemprop==1.7.1)\n",
            "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting typing-inspect>=0.7.1 (from typed-argument-parser>=1.6.1->chemprop==1.7.1)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/site-packages (from Werkzeug<3->chemprop==1.7.1) (2.1.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.8/site-packages (from importlib-metadata>=3.6.0->flask<=2.1.3,>=1.1.2->chemprop==1.7.1) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests>=2.25.0->sphinx>=3.1.2->chemprop==1.7.1) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests>=2.25.0->sphinx>=3.1.2->chemprop==1.7.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests>=2.25.0->sphinx>=3.1.2->chemprop==1.7.1) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests>=2.25.0->sphinx>=3.1.2->chemprop==1.7.1) (2024.8.30)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.7.1->typed-argument-parser>=1.6.1->chemprop==1.7.1)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.4.0->chemprop==1.7.1)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading chemprop-1.7.1-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading descriptastorus-2.8.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Flask-2.1.3-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas_flavor-0.7.0-py3-none-any.whl (8.4 kB)\n",
            "Downloading rdkit-2024.3.5-cp38-cp38-manylinux_2_28_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx-7.1.2-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl (797.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typed_argument_parser-1.10.1-py3-none-any.whl (30 kB)\n",
            "Downloading werkzeug-2.3.8-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.3/242.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alabaster-0.7.13-py3-none-any.whl (13 kB)\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.1/301.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
            "Downloading docutils-0.20.1-py3-none-any.whl (572 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.7/572.7 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fonttools-4.57.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading snowballstemmer-3.0.1-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_htmlhelp-2.0.1-py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.8/99.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading future-1.0.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.0/203.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_applehelp-1.0.4-py3-none-any.whl (120 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.6/120.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
            "Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xarray-2023.1.0-py3-none-any.whl (973 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.1/973.1 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mDEPRECATION: jupyter-server 2.0.0 has a non-standard dependency specifier jupyter-core!=~5.0,>=4.12. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of jupyter-server or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: py4j, mpmath, Werkzeug, threadpoolctl, sympy, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, snowballstemmer, pyparsing, protobuf, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, mypy-extensions, kiwisolver, joblib, itsdangerous, imagesize, future, fsspec, fonttools, filelock, docutils, docstring-parser, cycler, cloudpickle, click, alabaster, typing-inspect, triton, tensorboardX, sphinx, scipy, rdkit, nvidia-cusparse-cu12, nvidia-cudnn-cu12, flask, contourpy, xarray, typed-argument-parser, sphinxcontrib-jquery, scikit-learn, nvidia-cusolver-cu12, matplotlib, hyperopt, torch, sphinx-rtd-theme, pandas-flavor, descriptastorus, chemprop\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.3\n",
            "    Uninstalling numpy-1.22.3:\n",
            "      Successfully uninstalled numpy-1.22.3\n",
            "Successfully installed Werkzeug-2.3.8 alabaster-0.7.13 chemprop-1.7.1 click-8.1.8 cloudpickle-3.1.1 contourpy-1.1.1 cycler-0.12.1 descriptastorus-2.8.0 docstring-parser-0.17.0 docutils-0.20.1 filelock-3.16.1 flask-2.1.3 fonttools-4.57.0 fsspec-2025.3.0 future-1.0.0 hyperopt-0.2.7 imagesize-1.4.1 itsdangerous-2.2.0 joblib-1.4.2 kiwisolver-1.4.7 matplotlib-3.7.5 mpmath-1.3.0 mypy-extensions-1.1.0 networkx-3.1 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pandas-flavor-0.7.0 pillow-10.4.0 protobuf-5.29.5 py4j-0.10.9.9 pyparsing-3.1.4 rdkit-2024.3.5 scikit-learn-1.3.2 scipy-1.10.1 snowballstemmer-3.0.1 sphinx-7.1.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-applehelp-1.0.4 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.1 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 sphinxcontrib-serializinghtml-1.1.5 sympy-1.13.3 tensorboardX-2.6.2.2 threadpoolctl-3.5.0 torch-2.4.1 triton-3.0.0 typed-argument-parser-1.10.1 typing-inspect-0.9.0 xarray-2023.1.0\n"
          ]
        }
      ],
      "source": [
        "!python3.8 -m pip install chemprop==1.7.1 numpy==1.24.4 # must specify numpy version > 1.22 to avoid C-API import error; should take ~3 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf7z5r2aQiyv",
        "outputId": "bb1800b9-5a82-43e4-d512-2f6ffabe8c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/jupyter_client/manager.py:132: UserWarning: Setting kernel_cmd is deprecated, use kernel_spec to start different kernels.\n",
            "  warnings.warn(\"Setting kernel_cmd is deprecated, use kernel_spec to \"\n"
          ]
        }
      ],
      "source": [
        "# Start a persistent Python-3.8 REPL behind the scenes to run everything in for the below cell magic\n",
        "# Otherwise (with the cell magic only), variables and package imports do not carry over from cell to cell\n",
        "\n",
        "from IPython.core.magic import register_cell_magic\n",
        "from jupyter_client import KernelManager\n",
        "import json, atexit, textwrap\n",
        "\n",
        "km = KernelManager(kernel_name=\"python3\")      # path resolves to python3.8 if on $PATH\n",
        "km.kernel_cmd = [\"python3.8\", \"-m\", \"ipykernel_launcher\", \"-f\", \"{connection_file}\"]\n",
        "km.start_kernel()\n",
        "\n",
        "kc = km.client()\n",
        "kc.start_channels()\n",
        "\n",
        "@atexit.register\n",
        "def _clean():\n",
        "    kc.stop_channels()\n",
        "    km.shutdown_kernel(now=True)\n",
        "\n",
        "# Create new cell magic for cells to run their code through a Python 3.8 interpreter instead of the Colab default Python\n",
        "\n",
        "@register_cell_magic\n",
        "def py38(line, cell):\n",
        "    # send code, wait for reply, print results\n",
        "    msg_id = kc.execute(textwrap.dedent(cell))\n",
        "    while True:\n",
        "        msg = kc.get_iopub_msg()\n",
        "        if msg['parent_header'].get('msg_id') == msg_id:\n",
        "            if msg['msg_type'] == 'stream':\n",
        "                print(msg['content']['text'], end=\"\")\n",
        "            elif msg['msg_type'] == 'error':\n",
        "                print(\"\\n\".join(msg['content']['traceback']))\n",
        "            elif msg['msg_type'] == 'execute_result':\n",
        "                print(msg['content']['data']['text/plain'])\n",
        "            elif msg['msg_type'] == 'status' and msg['content']['execution_state'] == 'idle':\n",
        "                break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WGZxehx_44Rj"
      },
      "outputs": [],
      "source": [
        "%%py38\n",
        "import chemprop # sys.path.append('/usr/local/lib/python3.8/site-packages/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Zn9uWLe0Sv5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ac2b3a6-28de-4ad8-9c47-0afd7cad4058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'uvvisml'...\n",
            "remote: Enumerating objects: 147, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 147 (delta 11), reused 6 (delta 6), pack-reused 123 (from 1)\u001b[K\n",
            "Receiving objects: 100% (147/147), 9.28 MiB | 7.17 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/learningmatter-mit/uvvisml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TiQ_yhiUQ7hr"
      },
      "outputs": [],
      "source": [
        "%%py38\n",
        "import pandas as pd\n",
        "import os\n",
        "os.chdir('uvvisml/uvvisml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "V6GJHtwaRA0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cce25db-39a2-4447-8bef-015a1cecd8ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-30 14:44:27--  https://zenodo.org/record/5573027/files/models.tar.gz\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.45.92, 188.185.48.194, 188.185.43.25, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.45.92|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/5573027/files/models.tar.gz [following]\n",
            "--2025-07-30 14:44:28--  https://zenodo.org/records/5573027/files/models.tar.gz\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 651010218 (621M) [application/octet-stream]\n",
            "Saving to: ‘models.tar.gz’\n",
            "\n",
            "models.tar.gz       100%[===================>] 620.85M  2.68MB/s    in 8m 4s   \n",
            "\n",
            "2025-07-30 14:52:32 (1.28 MB/s) - ‘models.tar.gz’ saved [651010218/651010218]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!cd uvvisml/uvvisml; bash get_model_files.sh # may take ~2-10 minutes (download speeds from Zenodo are typically ~1-5MB/s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ67V6hr_6Yz"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pqtogu2T7OTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ecee211-77c4-42e3-82c3-9fd98680b7d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 smiles  ... peakwavs_max\n",
            "0                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        376.0\n",
            "1                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        392.0\n",
            "2                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        396.0\n",
            "3                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        400.0\n",
            "4                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        413.0\n",
            "...                                                 ...  ...          ...\n",
            "1705           c1cc2c3ccc[n+]4cccc(c5ccc[n+](c1)c25)c34  ...        424.0\n",
            "1706           c1cc2c3ccc[n+]4cccc(c5ccc[n+](c1)c25)c34  ...        432.0\n",
            "1707  COc1cc(C)c(-c2cc(-c3c(C)cc(OC)cc3C)c3ccc4c(-c5...  ...        367.0\n",
            "1708  N#Cc1c(N2CCCCC2)cc(-c2cccc3ccccc23)c2c1-c1cccc...  ...        358.0\n",
            "1709        N#Cc1c(N2CCCC2)cc(-c2ccccc2)c2c1Cc1ccccc1-2  ...        382.0\n",
            "\n",
            "[1710 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "%%py38\n",
        "test_file = 'data/splits/lambda_max_abs/deep4chem/group_by_smiles/smiles_target_test.csv'\n",
        "df = pd.read_csv(test_file)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZktZ7DsAAMn"
      },
      "source": [
        "# Make Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qWycAVkKlS0"
      },
      "source": [
        "## Predict experimental peak with model trained on combined training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g-bD5g6JCCy"
      },
      "source": [
        "**Equivalent to command line:**\n",
        "\n",
        "python uvvisml/predict.py --test_file uvvisml/data/splits/lambda_max_abs/deep4chem/group_by_smiles/smiles_target_test.csv --property absorption_peak_nm_expt --method chemprop --preds_file test_preds.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5a1-7UXcJCss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e5c8a4d-4146-4f56-9b44-d23dec6c8272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training args\n",
            "/usr/local/lib/python3.8/site-packages/chemprop/utils.py:472: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "Setting molecule featurization parameters to default.\n",
            "Loading data\n",
            "\r0it [00:00, ?it/s]\r1710it [00:00, 203019.13it/s]\n",
            "100%|██████████| 1710/1710 [00:00<00:00, 108164.20it/s]\n",
            "Validating SMILES\n",
            "/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Test size = 1,710\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.8/site-packages/chemprop/utils.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
            "/usr/local/lib/python3.8/site-packages/chemprop/utils.py:417: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "Loading pretrained parameter \"ffn.7.weight\".\n",
            "Loading pretrained parameter \"ffn.7.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:06<03:43,  6.57s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:08<02:01,  3.68s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:08<01:09,  2.18s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:09<00:53,  1.72s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:10<00:40,  1.34s/it]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:10<00:29,  1.01s/it]\u001b[A\n",
            " 20%|██        | 7/35 [00:11<00:23,  1.19it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:11<00:17,  1.50it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:12<00:16,  1.58it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:12<00:14,  1.69it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:13<00:13,  1.76it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:13<00:13,  1.65it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:14<00:16,  1.30it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:15<00:12,  1.73it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:15<00:09,  2.01it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:15<00:07,  2.39it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:15<00:06,  2.89it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:16<00:07,  2.43it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:16<00:05,  2.77it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:16<00:05,  2.96it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:16<00:03,  3.54it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:17<00:02,  5.24it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:17<00:01,  7.13it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:17<00:01,  7.10it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:17<00:00,  7.12it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:17<00:00,  6.85it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:18<00:00,  8.33it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:18<00:00,  8.51it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:18<00:00,  8.91it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:18<01:14, 18.70s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "Loading pretrained parameter \"ffn.7.weight\".\n",
            "Loading pretrained parameter \"ffn.7.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:07<04:06,  7.25s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:07<01:43,  3.13s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:07<00:57,  1.79s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:08<00:43,  1.39s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:08<00:29,  1.03it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:09<00:22,  1.32it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:09<00:17,  1.62it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:09<00:13,  2.04it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:09<00:10,  2.57it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:10<00:06,  3.72it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:10<00:06,  3.39it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:10<00:06,  3.36it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:10<00:05,  3.79it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:11<00:04,  4.05it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:11<00:04,  4.48it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:11<00:03,  4.66it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:11<00:03,  5.06it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:11<00:03,  4.84it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:12<00:03,  3.80it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:12<00:02,  5.57it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:12<00:02,  5.53it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:12<00:01,  7.51it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:12<00:00,  8.74it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:13<00:01,  5.30it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:13<00:00,  6.53it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:13<00:00,  6.83it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:13<00:00,  6.89it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:13<00:00,  7.38it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:14<00:00,  7.60it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:33<00:48, 16.17s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "Loading pretrained parameter \"ffn.7.weight\".\n",
            "Loading pretrained parameter \"ffn.7.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:05<03:01,  5.33s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:05<01:17,  2.35s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:05<00:44,  1.40s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:06<00:30,  1.01it/s]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:06<00:23,  1.28it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:06<00:17,  1.63it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:07<00:15,  1.82it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:07<00:12,  2.25it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:07<00:10,  2.56it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:08<00:08,  3.00it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:08<00:07,  3.27it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:08<00:07,  2.94it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:09<00:09,  2.28it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:09<00:07,  2.83it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:09<00:07,  2.83it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:10<00:05,  3.43it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:10<00:04,  3.98it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:10<00:04,  3.78it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:10<00:03,  4.37it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:10<00:02,  5.22it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:11<00:03,  4.36it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:11<00:02,  5.90it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:11<00:01,  8.02it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:11<00:00,  8.81it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:12<00:01,  5.62it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:12<00:00,  6.63it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:12<00:00,  6.97it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:12<00:00,  7.51it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:46<00:29, 14.76s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "Loading pretrained parameter \"ffn.7.weight\".\n",
            "Loading pretrained parameter \"ffn.7.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:07<04:18,  7.61s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:07<01:46,  3.24s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:08<01:00,  1.90s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:08<00:41,  1.35s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:08<00:28,  1.04it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:09<00:20,  1.43it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:09<00:16,  1.71it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:09<00:12,  2.17it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:09<00:10,  2.42it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:10<00:08,  2.81it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:10<00:07,  3.41it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:10<00:07,  3.05it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:10<00:06,  3.48it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:11<00:05,  3.80it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:11<00:05,  3.53it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:11<00:04,  4.14it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:11<00:03,  4.59it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:11<00:03,  4.44it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:12<00:03,  4.63it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:12<00:03,  3.93it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:12<00:02,  6.00it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:12<00:01,  6.22it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:12<00:01,  7.95it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:13<00:00,  8.91it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:13<00:01,  4.42it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:13<00:00,  5.68it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:14<00:00,  6.74it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:14<00:00,  8.02it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [01:00<00:14, 14.72s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "Loading pretrained parameter \"ffn.7.weight\".\n",
            "Loading pretrained parameter \"ffn.7.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:04<02:49,  4.99s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:05<01:13,  2.23s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:05<00:29,  1.06it/s]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:05<00:22,  1.35it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:06<00:16,  1.71it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:06<00:14,  1.99it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:06<00:11,  2.36it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:07<00:09,  2.61it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:07<00:09,  2.66it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:07<00:08,  2.87it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:08<00:08,  2.82it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:08<00:08,  2.52it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:08<00:08,  2.59it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:09<00:07,  2.73it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:09<00:06,  2.99it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:09<00:05,  3.43it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:10<00:07,  2.35it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:10<00:05,  2.96it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:10<00:03,  3.88it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:10<00:02,  5.38it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:11<00:01,  7.11it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:11<00:00,  8.07it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:11<00:00,  7.60it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:11<00:00,  8.81it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:11<00:00,  9.08it/s]\u001b[A\n",
            "100%|██████████| 35/35 [00:12<00:00, 10.06it/s]\u001b[A\n",
            "100%|██████████| 5/5 [01:13<00:00, 14.65s/it]Saving predictions to /dev/null\n",
            "Elapsed time = 0:01:14\n",
            "\n",
            "                                                 smiles  ... peakwavs_max_pred\n",
            "0                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        378.089791\n",
            "1                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        388.387075\n",
            "2                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        394.557472\n",
            "3                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        400.817724\n",
            "4                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        410.902339\n",
            "...                                                 ...  ...               ...\n",
            "1705           c1cc2c3ccc[n+]4cccc(c5ccc[n+](c1)c25)c34  ...        425.084342\n",
            "1706           c1cc2c3ccc[n+]4cccc(c5ccc[n+](c1)c25)c34  ...        428.209718\n",
            "1707  COc1cc(C)c(-c2cc(-c3c(C)cc(OC)cc3C)c3ccc4c(-c5...  ...        370.200150\n",
            "1708  N#Cc1c(N2CCCCC2)cc(-c2cccc3ccccc23)c2c1-c1cccc...  ...        355.987562\n",
            "1709        N#Cc1c(N2CCCC2)cc(-c2ccccc2)c2c1Cc1ccccc1-2  ...        368.792145\n",
            "\n",
            "[1710 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "%%py38\n",
        "arguments = [\n",
        "  '--test_path', test_file,\n",
        "  '--preds_path', '/dev/null',\n",
        "  '--checkpoint_dir', 'models/lambda_max_abs/chemprop/combined/production/fold_0',\n",
        "  '--number_of_molecules', '2',\n",
        "  #'--gpu', '0'\n",
        "]\n",
        "\n",
        "args = chemprop.args.PredictArgs().parse_args(arguments)\n",
        "preds = chemprop.train.make_predictions(args=args)\n",
        "\n",
        "preds = [x[0] for x in preds]\n",
        "df['peakwavs_max_pred'] = preds\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE9Tof7UK8cI"
      },
      "source": [
        "## Predict TDDFT peak in vacuum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zypfNmVzJKEc"
      },
      "source": [
        "**Equivalent to command line:**\n",
        "\n",
        "python uvvisml/predict.py --test_file uvvisml/data/splits/lambda_max_abs/deep4chem/group_by_smiles/smiles_target_test.csv --property vertical_excitation_eV_tddft --method chemprop --preds_file test_preds.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kswx6y_uJHqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb08e1f-202d-4afe-d24a-bde8b8df4b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training args\n",
            "/usr/local/lib/python3.8/site-packages/chemprop/utils.py:472: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "Setting molecule featurization parameters to default.\n",
            "Loading data\n",
            "1710it [00:00, 170273.49it/s]\n",
            "100%|██████████| 1710/1710 [00:00<00:00, 115144.89it/s]\n",
            "/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Validating SMILES\n",
            "Test size = 1,710\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.8/site-packages/chemprop/utils.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
            "/usr/local/lib/python3.8/site-packages/chemprop/utils.py:417: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:06<03:56,  6.95s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:07<01:39,  3.03s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:07<00:56,  1.77s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:07<00:38,  1.24s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:08<00:28,  1.07it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:08<00:20,  1.39it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:09<00:17,  1.56it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:09<00:14,  1.91it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:09<00:11,  2.32it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:09<00:09,  2.65it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:10<00:08,  2.81it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:10<00:08,  2.73it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:11<00:09,  2.32it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:11<00:07,  2.85it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:11<00:06,  3.09it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:11<00:05,  3.62it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:12<00:05,  3.59it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:12<00:04,  4.05it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:12<00:03,  4.48it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:12<00:03,  4.95it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:12<00:03,  4.44it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:12<00:02,  5.29it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:13<00:02,  5.09it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:13<00:01,  6.47it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:13<00:01,  6.82it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:13<00:01,  6.96it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:13<00:01,  6.38it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:14<00:01,  5.15it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:14<00:00,  5.69it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:14<00:00,  6.01it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:14<00:00,  6.01it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:14<00:00,  5.86it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:14<00:00,  6.12it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:15<01:00, 15.19s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:06<03:30,  6.19s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:07<01:40,  3.04s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:07<00:57,  1.80s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:07<00:39,  1.28s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:08<00:27,  1.07it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:08<00:21,  1.35it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:09<00:18,  1.51it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:09<00:14,  1.87it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:09<00:11,  2.17it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:09<00:10,  2.42it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:10<00:08,  2.83it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:10<00:07,  3.13it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:10<00:07,  3.13it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:10<00:06,  3.41it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:11<00:05,  3.59it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:11<00:04,  4.18it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:11<00:04,  4.02it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:11<00:04,  3.54it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:12<00:03,  4.06it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:12<00:03,  4.42it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:12<00:02,  5.29it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:12<00:02,  6.10it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:12<00:02,  5.55it/s]\u001b[A\n",
            " 69%|██████▊   | 24/35 [00:12<00:01,  6.40it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:13<00:01,  6.15it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:13<00:01,  6.38it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:13<00:01,  5.78it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:13<00:01,  5.69it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:13<00:00,  6.07it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:13<00:00,  6.18it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:14<00:00,  5.94it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:14<00:00,  6.15it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:14<00:00,  6.13it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:30<00:44, 14.97s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:07<04:08,  7.32s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:07<01:47,  3.27s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:07<01:00,  1.88s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:08<00:40,  1.32s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:08<00:28,  1.06it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:09<00:21,  1.36it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:09<00:20,  1.39it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:10<00:15,  1.69it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:10<00:12,  2.15it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:10<00:09,  2.70it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:10<00:07,  3.10it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:10<00:06,  3.33it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:11<00:06,  3.26it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:11<00:05,  3.76it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:11<00:05,  3.53it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:11<00:04,  4.14it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:11<00:03,  4.54it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:12<00:04,  3.66it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:12<00:03,  4.20it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:12<00:03,  4.45it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:12<00:02,  5.32it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:12<00:02,  6.17it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:13<00:02,  5.72it/s]\u001b[A\n",
            " 69%|██████▊   | 24/35 [00:13<00:01,  6.50it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:13<00:01,  6.42it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:13<00:01,  6.82it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:13<00:01,  5.81it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:14<00:01,  5.76it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:14<00:00,  6.26it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:14<00:00,  6.40it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:14<00:00,  6.07it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:14<00:00,  6.41it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:14<00:00,  6.20it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:45<00:30, 15.11s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:07<04:02,  7.13s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:07<01:45,  3.20s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:07<01:01,  1.91s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:08<00:39,  1.27s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:08<00:29,  1.02it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:08<00:20,  1.41it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:09<00:16,  1.74it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:09<00:12,  2.21it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:09<00:09,  2.80it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:09<00:07,  3.48it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:09<00:06,  3.68it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:10<00:07,  3.17it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:10<00:07,  3.08it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:10<00:06,  3.49it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:11<00:05,  3.53it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:11<00:04,  4.11it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:11<00:03,  4.57it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:11<00:04,  4.09it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:12<00:04,  3.71it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:12<00:03,  4.20it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:12<00:02,  4.76it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:12<00:02,  5.37it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:12<00:02,  5.02it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:12<00:01,  6.76it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:13<00:01,  7.10it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:13<00:01,  6.15it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:13<00:01,  5.54it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:13<00:01,  5.54it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:13<00:00,  6.09it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:13<00:00,  6.14it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:14<00:00,  6.17it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:14<00:00,  6.48it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:14<00:00,  6.77it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [01:00<00:14, 14.97s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:08<04:34,  8.07s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:08<01:56,  3.54s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:08<01:05,  2.06s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:09<00:44,  1.44s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:09<00:29,  1.02it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:09<00:20,  1.39it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:10<00:17,  1.57it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:10<00:13,  2.07it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:10<00:10,  2.60it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:10<00:07,  3.27it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:10<00:06,  3.53it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:11<00:06,  3.41it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:11<00:06,  3.54it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:11<00:05,  3.81it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:11<00:05,  3.46it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:12<00:04,  4.16it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:12<00:04,  4.20it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:12<00:03,  4.44it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:12<00:03,  4.93it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:12<00:03,  3.96it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:13<00:02,  5.52it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:13<00:02,  5.47it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:13<00:01,  6.95it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:13<00:01,  7.21it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:13<00:01,  7.22it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:14<00:01,  5.28it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:14<00:01,  5.49it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:14<00:00,  6.05it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:14<00:00,  6.35it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:14<00:00,  6.30it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:14<00:00,  6.39it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:14<00:00,  6.69it/s]\u001b[A\n",
            "100%|██████████| 5/5 [01:15<00:00, 15.08s/it]\n",
            "Saving predictions to /dev/null\n",
            "Elapsed time = 0:01:16\n",
            "                                                 smiles  ... peakwavs_max_pred\n",
            "0                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        309.772465\n",
            "1                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        309.772465\n",
            "2                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        309.772465\n",
            "3                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        309.772465\n",
            "4                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        309.772465\n",
            "...                                                 ...  ...               ...\n",
            "1705           c1cc2c3ccc[n+]4cccc(c5ccc[n+](c1)c25)c34  ...        346.755560\n",
            "1706           c1cc2c3ccc[n+]4cccc(c5ccc[n+](c1)c25)c34  ...        346.755560\n",
            "1707  COc1cc(C)c(-c2cc(-c3c(C)cc(OC)cc3C)c3ccc4c(-c5...  ...        318.639569\n",
            "1708  N#Cc1c(N2CCCCC2)cc(-c2cccc3ccccc23)c2c1-c1cccc...  ...        310.870571\n",
            "1709        N#Cc1c(N2CCCC2)cc(-c2ccccc2)c2c1Cc1ccccc1-2  ...        317.668449\n",
            "\n",
            "[1710 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "%%py38\n",
        "arguments = [\n",
        "  '--test_path', test_file,\n",
        "  '--preds_path', '/dev/null',\n",
        "  '--checkpoint_dir', 'models/lambda_max_abs_wb97xd3/chemprop/all_wb97xd3/production/fold_0',\n",
        "  '--number_of_molecules', '1',\n",
        "  #'--gpu', '0'\n",
        "]\n",
        "\n",
        "args = chemprop.args.PredictArgs().parse_args(arguments)\n",
        "preds = chemprop.train.make_predictions(args=args)\n",
        "\n",
        "preds = [x[0] for x in preds] # predictions are in eV\n",
        "df['peakwavs_max_pred'] = preds\n",
        "df['peakwavs_max_pred'] = 1240/df['peakwavs_max_pred'] # convert from eV to nm\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54x-eGwxLEZ1"
      },
      "source": [
        "## Predict experimental peak with model trained on Deep4Chem training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_68WpdSDI110"
      },
      "source": [
        "**Equivalent to command line:**\n",
        "\n",
        "python uvvisml/predict.py --test_file uvvisml/data/splits/lambda_max_abs/deep4chem/group_by_smiles/smiles_target_test.csv --property absorption_peak_nm_expt --method chemprop --preds_file test_preds.csv --train_dataset deep4chem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ICmfelGD7rcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa68e85a-432c-4e97-cee5-83324cd510c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training args\n",
            "/usr/local/lib/python3.8/site-packages/chemprop/utils.py:472: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "Setting molecule featurization parameters to default.\n",
            "Loading data\n",
            "1710it [00:00, 159019.57it/s]\n",
            "100%|██████████| 1710/1710 [00:00<00:00, 101888.82it/s]\n",
            "/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Validating SMILES\n",
            "Test size = 1,710\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.8/site-packages/chemprop/utils.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
            "/usr/local/lib/python3.8/site-packages/chemprop/utils.py:417: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:07<03:59,  7.04s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:07<01:42,  3.10s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:07<00:57,  1.81s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:08<00:39,  1.27s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:08<00:27,  1.08it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:08<00:21,  1.35it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:09<00:18,  1.48it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:09<00:14,  1.85it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:09<00:11,  2.22it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:10<00:09,  2.56it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:10<00:08,  2.77it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:10<00:08,  2.64it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:11<00:08,  2.53it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:11<00:07,  2.71it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:11<00:07,  2.56it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:12<00:06,  2.86it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:12<00:06,  2.85it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:12<00:05,  3.03it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:13<00:04,  3.46it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:13<00:04,  3.25it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:13<00:03,  3.62it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:13<00:03,  3.95it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:14<00:03,  3.33it/s]\u001b[A\n",
            " 69%|██████▊   | 24/35 [00:14<00:02,  3.73it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:14<00:02,  4.08it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:14<00:02,  4.09it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:15<00:01,  4.04it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:16<00:03,  2.20it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:16<00:02,  2.11it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:16<00:02,  2.23it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:17<00:01,  2.20it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:17<00:01,  2.14it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:18<00:00,  2.20it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:18<00:00,  2.30it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:19<01:18, 19.52s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:05<03:10,  5.61s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:06<01:25,  2.58s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:06<00:48,  1.53s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:06<00:32,  1.06s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:07<00:24,  1.24it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:07<00:18,  1.55it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:07<00:16,  1.74it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:08<00:13,  2.04it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:08<00:10,  2.37it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:08<00:10,  2.48it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:09<00:08,  2.70it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:09<00:08,  2.70it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:09<00:09,  2.34it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:10<00:09,  2.26it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:11<00:09,  2.02it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:11<00:09,  1.95it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:12<00:09,  1.86it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:12<00:10,  1.65it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:13<00:08,  1.91it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:13<00:07,  2.08it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:13<00:05,  2.49it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:14<00:04,  2.94it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:14<00:04,  2.94it/s]\u001b[A\n",
            " 69%|██████▊   | 24/35 [00:14<00:03,  3.36it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:14<00:02,  3.75it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:15<00:03,  2.52it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:15<00:02,  2.79it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:16<00:02,  2.75it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:16<00:02,  2.86it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:16<00:01,  3.11it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:17<00:01,  3.29it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:17<00:00,  3.27it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:17<00:00,  3.32it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:17<00:00,  3.32it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:37<00:56, 18.89s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:14<08:11, 14.46s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:15<03:28,  6.31s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:15<01:55,  3.59s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:16<01:15,  2.43s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:16<00:50,  1.70s/it]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:17<00:39,  1.36s/it]\u001b[A\n",
            " 20%|██        | 7/35 [00:17<00:32,  1.15s/it]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:18<00:23,  1.16it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:18<00:17,  1.48it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:18<00:14,  1.75it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:19<00:12,  1.99it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:19<00:10,  2.14it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:20<00:11,  1.95it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:20<00:09,  2.24it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:20<00:08,  2.26it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:21<00:07,  2.58it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:21<00:06,  2.71it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:21<00:06,  2.70it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:22<00:05,  2.87it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:22<00:04,  3.09it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:22<00:03,  3.53it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:22<00:03,  3.65it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:23<00:04,  2.61it/s]\u001b[A\n",
            " 69%|██████▊   | 24/35 [00:23<00:04,  2.69it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:24<00:03,  2.88it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:24<00:04,  2.01it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:25<00:03,  2.01it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:26<00:03,  1.84it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:26<00:03,  1.93it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:26<00:02,  2.22it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:27<00:01,  2.41it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:27<00:01,  2.44it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:27<00:00,  2.63it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:28<00:00,  2.70it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [01:06<00:46, 23.37s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:05<03:19,  5.87s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:06<01:29,  2.72s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:06<00:51,  1.60s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:07<00:37,  1.22s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:07<00:26,  1.13it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:08<00:22,  1.30it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:08<00:21,  1.29it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:09<00:16,  1.61it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:09<00:15,  1.73it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:10<00:14,  1.74it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:10<00:12,  1.85it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:11<00:14,  1.61it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:12<00:12,  1.72it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:12<00:10,  2.01it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:12<00:09,  2.14it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:12<00:07,  2.44it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:13<00:07,  2.55it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:13<00:06,  2.57it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:13<00:05,  2.83it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:14<00:06,  2.20it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:14<00:05,  2.65it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:15<00:04,  3.09it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:15<00:04,  2.74it/s]\u001b[A\n",
            " 69%|██████▊   | 24/35 [00:15<00:03,  3.20it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:15<00:02,  3.63it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:16<00:02,  3.36it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:16<00:02,  3.32it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:17<00:02,  2.60it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:17<00:02,  2.66it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:17<00:01,  2.92it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:18<00:01,  3.08it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:18<00:00,  3.09it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:18<00:00,  3.20it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:18<00:00,  3.25it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [01:26<00:21, 21.84s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:06<03:57,  6.97s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:07<01:41,  3.08s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:07<00:58,  1.82s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:08<00:38,  1.25s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:08<00:28,  1.07it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:09<00:24,  1.18it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:09<00:21,  1.31it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:10<00:17,  1.55it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:10<00:13,  1.89it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:10<00:11,  2.22it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:11<00:10,  2.29it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:11<00:09,  2.42it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:11<00:10,  2.17it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:12<00:10,  1.94it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:13<00:09,  2.03it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:13<00:08,  2.36it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:13<00:07,  2.52it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:14<00:07,  2.33it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:15<00:09,  1.71it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:15<00:07,  2.11it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:15<00:05,  2.58it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:15<00:04,  2.99it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:16<00:04,  2.80it/s]\u001b[A\n",
            " 69%|██████▊   | 24/35 [00:16<00:03,  2.90it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:16<00:03,  3.06it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:17<00:03,  2.96it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:17<00:03,  2.34it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:18<00:03,  2.04it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:18<00:03,  1.98it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:19<00:02,  2.13it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:19<00:01,  2.35it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:19<00:01,  2.51it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:20<00:00,  2.73it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:20<00:00,  2.91it/s]\u001b[A\n",
            "100%|██████████| 5/5 [01:47<00:00, 21.44s/it]Saving predictions to /dev/null\n",
            "Elapsed time = 0:01:48\n",
            "\n",
            "                                                 smiles  ... peakwavs_max_pred\n",
            "0                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        382.903437\n",
            "1                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        395.478472\n",
            "2                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        400.821401\n",
            "3                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        418.106349\n",
            "4                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        426.706045\n",
            "...                                                 ...  ...               ...\n",
            "1705           c1cc2c3ccc[n+]4cccc(c5ccc[n+](c1)c25)c34  ...        474.872657\n",
            "1706           c1cc2c3ccc[n+]4cccc(c5ccc[n+](c1)c25)c34  ...        466.555822\n",
            "1707  COc1cc(C)c(-c2cc(-c3c(C)cc(OC)cc3C)c3ccc4c(-c5...  ...        365.403474\n",
            "1708  N#Cc1c(N2CCCCC2)cc(-c2cccc3ccccc23)c2c1-c1cccc...  ...        349.835862\n",
            "1709        N#Cc1c(N2CCCC2)cc(-c2ccccc2)c2c1Cc1ccccc1-2  ...        352.427928\n",
            "\n",
            "[1710 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "%%py38\n",
        "arguments = [\n",
        "  '--test_path', test_file,\n",
        "  '--preds_path', '/dev/null',\n",
        "  '--checkpoint_dir', 'models/lambda_max_abs/chemprop/deep4chem/production/fold_0',\n",
        "  '--number_of_molecules', '2',\n",
        "  #'--gpu', '0'\n",
        "]\n",
        "\n",
        "args = chemprop.args.PredictArgs().parse_args(arguments)\n",
        "preds = chemprop.train.make_predictions(args=args)\n",
        "\n",
        "preds = [x[0] for x in preds]\n",
        "df['peakwavs_max_pred'] = preds\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha40pwkQKZ2F"
      },
      "source": [
        "## Predict experimental peak with multi-fidelity model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IpT4M5CJjln"
      },
      "source": [
        "**Equivalent to command line:**\n",
        "\n",
        "python uvvisml/predict.py --test_file uvvisml/data/splits/lambda_max_abs/deep4chem/group_by_smiles/smiles_target_test.csv --property absorption_peak_nm_expt --method chemprop_tddft --preds_file test_preds.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6OuRAYrKJjaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caab8de0-0689-49a5-d192-72157bffc9fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training args\n",
            "/usr/local/lib/python3.8/site-packages/chemprop/utils.py:472: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "Setting molecule featurization parameters to default.\n",
            "Loading data\n",
            "1710it [00:00, 184785.38it/s]\n",
            "100%|██████████| 1710/1710 [00:00<00:00, 111460.49it/s]\n",
            "/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Validating SMILES\n",
            "Test size = 1,710\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.8/site-packages/chemprop/utils.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
            "/usr/local/lib/python3.8/site-packages/chemprop/utils.py:417: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:07<03:58,  7.02s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:07<01:39,  3.02s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:07<00:55,  1.72s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:07<00:35,  1.15s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:07<00:24,  1.25it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:08<00:17,  1.64it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:08<00:15,  1.82it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:08<00:12,  2.14it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:09<00:12,  2.10it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:09<00:09,  2.57it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:09<00:09,  2.50it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:10<00:10,  2.18it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:11<00:10,  2.06it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:11<00:09,  2.18it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:11<00:09,  2.20it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:12<00:07,  2.58it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:12<00:08,  2.21it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:13<00:07,  2.32it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:13<00:06,  2.60it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:13<00:05,  2.81it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:13<00:03,  4.16it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:14<00:02,  4.37it/s]\u001b[A\n",
            " 69%|██████▊   | 24/35 [00:14<00:02,  5.10it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:14<00:01,  5.22it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:14<00:01,  5.88it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:14<00:01,  6.39it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:14<00:01,  5.38it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:15<00:01,  5.47it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:15<00:00,  6.09it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:15<00:00,  6.34it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:15<00:00,  6.31it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:15<00:00,  5.92it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:15<00:00,  5.96it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:16<01:05, 16.33s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:06<03:37,  6.40s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:06<01:30,  2.75s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:06<00:50,  1.57s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:07<00:32,  1.05s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:07<00:22,  1.34it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:07<00:16,  1.73it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:07<00:15,  1.82it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:08<00:12,  2.17it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:08<00:11,  2.21it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:08<00:10,  2.46it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:09<00:09,  2.58it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:09<00:09,  2.45it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:10<00:09,  2.23it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:10<00:08,  2.46it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:11<00:07,  2.52it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:11<00:06,  2.80it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:11<00:06,  2.86it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:11<00:04,  3.43it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:11<00:04,  3.85it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:12<00:03,  3.85it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:12<00:02,  5.40it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:12<00:02,  5.35it/s]\u001b[A\n",
            " 69%|██████▊   | 24/35 [00:12<00:01,  6.09it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:12<00:01,  6.09it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:12<00:01,  6.39it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:13<00:01,  6.22it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:13<00:01,  4.78it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:13<00:01,  4.82it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:13<00:00,  5.32it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:13<00:00,  5.65it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:14<00:00,  5.77it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:14<00:00,  5.82it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:14<00:00,  6.23it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:31<00:46, 15.47s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:06<03:40,  6.49s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:06<01:33,  2.82s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:07<00:54,  1.69s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:07<00:36,  1.18s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:07<00:26,  1.14it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:08<00:20,  1.43it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:08<00:17,  1.63it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:08<00:14,  1.92it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:09<00:11,  2.17it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:09<00:09,  2.58it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:10<00:11,  2.06it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:10<00:10,  2.10it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:11<00:11,  1.89it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:11<00:09,  2.15it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:11<00:07,  2.52it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:12<00:06,  3.12it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:12<00:05,  3.37it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:12<00:04,  3.92it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:12<00:05,  3.14it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:13<00:04,  3.65it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:13<00:03,  4.41it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:13<00:02,  5.23it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:13<00:02,  5.50it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:13<00:01,  6.56it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:13<00:01,  6.91it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:13<00:01,  6.33it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:14<00:01,  5.67it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:14<00:01,  5.77it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:14<00:00,  6.31it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:14<00:00,  6.76it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:14<00:00,  6.42it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:14<00:00,  6.34it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:15<00:00,  6.65it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:46<00:30, 15.47s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:07<04:00,  7.08s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:07<01:42,  3.11s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:07<00:58,  1.84s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:08<00:39,  1.28s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:08<00:27,  1.10it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:08<00:21,  1.38it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:09<00:17,  1.58it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:09<00:13,  1.99it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:09<00:12,  2.10it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:09<00:09,  2.69it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:10<00:08,  2.95it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:10<00:07,  3.20it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:10<00:07,  3.00it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:11<00:06,  3.49it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:11<00:05,  3.44it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:11<00:04,  4.03it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:11<00:04,  3.97it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:12<00:04,  4.06it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:12<00:04,  3.94it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:12<00:03,  4.54it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:12<00:02,  5.40it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:12<00:02,  6.14it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:12<00:02,  5.51it/s]\u001b[A\n",
            " 69%|██████▊   | 24/35 [00:12<00:01,  6.36it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:13<00:01,  6.16it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:13<00:01,  6.78it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:13<00:01,  6.55it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:13<00:01,  5.73it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:13<00:01,  5.83it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:13<00:00,  6.39it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:14<00:00,  6.37it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:14<00:00,  6.25it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:14<00:00,  5.82it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:14<00:00,  6.26it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [01:01<00:15, 15.28s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:06<03:42,  6.53s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:07<01:44,  3.17s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:07<00:57,  1.81s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:07<00:39,  1.27s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:08<00:26,  1.11it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:08<00:21,  1.37it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:08<00:16,  1.68it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:09<00:12,  2.15it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:09<00:09,  2.73it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:09<00:07,  3.31it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:09<00:06,  3.78it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:09<00:06,  3.69it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:10<00:06,  3.53it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:10<00:05,  3.57it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:10<00:05,  3.75it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:10<00:05,  3.77it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:11<00:04,  3.72it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:11<00:06,  2.47it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:12<00:05,  2.91it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:12<00:04,  3.40it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:12<00:03,  4.19it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:12<00:02,  4.72it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:12<00:02,  4.44it/s]\u001b[A\n",
            " 69%|██████▊   | 24/35 [00:13<00:02,  5.05it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:13<00:01,  5.89it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:13<00:01,  5.02it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:13<00:01,  5.55it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:13<00:01,  5.21it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:13<00:01,  5.45it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:14<00:00,  5.79it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:14<00:00,  6.14it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:14<00:00,  6.16it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:14<00:00,  6.39it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:14<00:00,  5.97it/s]\u001b[A\n",
            "100%|██████████| 5/5 [01:16<00:00, 15.35s/it]\n",
            "Saving predictions to test_tddft_preds.csv\n",
            "Elapsed time = 0:01:17\n",
            "Loading training args\n",
            "Setting molecule featurization parameters to default.\n",
            "Loading data\n",
            "1710it [00:00, 154458.06it/s]\n",
            "100%|██████████| 1710/1710 [00:00<00:00, 6164.79it/s]\n",
            "Validating SMILES\n",
            "Test size = 1,710\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "Loading pretrained parameter \"ffn.7.weight\".\n",
            "Loading pretrained parameter \"ffn.7.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:08<05:02,  8.89s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:12<03:03,  5.57s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:15<02:24,  4.53s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:19<02:10,  4.22s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:21<01:50,  3.67s/it]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:25<01:41,  3.49s/it]\u001b[A\n",
            " 20%|██        | 7/35 [00:30<01:52,  4.02s/it]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:32<01:33,  3.45s/it]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:34<01:20,  3.11s/it]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:36<01:10,  2.83s/it]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:39<01:05,  2.72s/it]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:43<01:13,  3.18s/it]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:47<01:14,  3.39s/it]\u001b[A\n",
            " 40%|████      | 14/35 [00:50<01:06,  3.16s/it]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:54<01:08,  3.42s/it]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:57<01:04,  3.37s/it]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:59<00:54,  3.03s/it]\u001b[A\n",
            " 51%|█████▏    | 18/35 [01:01<00:47,  2.81s/it]\u001b[A\n",
            " 54%|█████▍    | 19/35 [01:03<00:41,  2.56s/it]\u001b[A\n",
            " 57%|█████▋    | 20/35 [01:06<00:39,  2.64s/it]\u001b[A\n",
            " 60%|██████    | 21/35 [01:10<00:41,  2.93s/it]\u001b[A\n",
            " 63%|██████▎   | 22/35 [01:12<00:36,  2.81s/it]\u001b[A\n",
            " 66%|██████▌   | 23/35 [01:16<00:37,  3.16s/it]\u001b[A\n",
            " 69%|██████▊   | 24/35 [01:19<00:31,  2.86s/it]\u001b[A\n",
            " 71%|███████▏  | 25/35 [01:21<00:27,  2.76s/it]\u001b[A\n",
            " 74%|███████▍  | 26/35 [01:24<00:26,  2.95s/it]\u001b[A\n",
            " 77%|███████▋  | 27/35 [01:27<00:22,  2.87s/it]\u001b[A\n",
            " 80%|████████  | 28/35 [01:32<00:23,  3.35s/it]\u001b[A\n",
            " 83%|████████▎ | 29/35 [01:37<00:23,  3.91s/it]\u001b[A\n",
            " 86%|████████▌ | 30/35 [01:40<00:17,  3.55s/it]\u001b[A\n",
            " 89%|████████▊ | 31/35 [01:42<00:13,  3.33s/it]\u001b[A\n",
            " 91%|█████████▏| 32/35 [01:46<00:10,  3.41s/it]\u001b[A\n",
            " 94%|█████████▍| 33/35 [01:50<00:07,  3.67s/it]\u001b[A\n",
            " 97%|█████████▋| 34/35 [01:53<00:03,  3.43s/it]\u001b[A\n",
            "100%|██████████| 35/35 [01:54<00:00,  2.58s/it]\u001b[A\n",
            " 20%|██        | 1/5 [01:55<07:40, 115.20s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "Loading pretrained parameter \"ffn.7.weight\".\n",
            "Loading pretrained parameter \"ffn.7.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:10<05:47, 10.23s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:13<03:18,  6.02s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:15<02:18,  4.31s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:20<02:18,  4.47s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:23<02:01,  4.04s/it]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:26<01:49,  3.78s/it]\u001b[A\n",
            " 20%|██        | 7/35 [00:31<01:51,  3.97s/it]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:35<01:46,  3.93s/it]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:37<01:30,  3.48s/it]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:39<01:18,  3.12s/it]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:42<01:11,  2.98s/it]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:45<01:09,  3.02s/it]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:51<01:26,  3.93s/it]\u001b[A\n",
            " 40%|████      | 14/35 [00:55<01:18,  3.76s/it]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:58<01:15,  3.77s/it]\u001b[A\n",
            " 46%|████▌     | 16/35 [01:01<01:07,  3.58s/it]\u001b[A\n",
            " 49%|████▊     | 17/35 [01:04<01:00,  3.35s/it]\u001b[A\n",
            " 51%|█████▏    | 18/35 [01:07<00:53,  3.13s/it]\u001b[A\n",
            " 54%|█████▍    | 19/35 [01:09<00:46,  2.89s/it]\u001b[A\n",
            " 57%|█████▋    | 20/35 [01:13<00:46,  3.10s/it]\u001b[A\n",
            " 60%|██████    | 21/35 [01:16<00:44,  3.19s/it]\u001b[A\n",
            " 63%|██████▎   | 22/35 [01:19<00:41,  3.16s/it]\u001b[A\n",
            " 66%|██████▌   | 23/35 [01:24<00:42,  3.55s/it]\u001b[A\n",
            " 69%|██████▊   | 24/35 [01:28<00:40,  3.69s/it]\u001b[A\n",
            " 71%|███████▏  | 25/35 [01:31<00:35,  3.53s/it]\u001b[A\n",
            " 74%|███████▍  | 26/35 [01:34<00:29,  3.29s/it]\u001b[A\n",
            " 77%|███████▋  | 27/35 [01:36<00:25,  3.14s/it]\u001b[A\n",
            " 80%|████████  | 28/35 [01:42<00:27,  3.92s/it]\u001b[A\n",
            " 83%|████████▎ | 29/35 [01:47<00:24,  4.13s/it]\u001b[A\n",
            " 86%|████████▌ | 30/35 [01:50<00:18,  3.80s/it]\u001b[A\n",
            " 89%|████████▊ | 31/35 [01:53<00:14,  3.54s/it]\u001b[A\n",
            " 91%|█████████▏| 32/35 [01:58<00:12,  4.06s/it]\u001b[A\n",
            " 94%|█████████▍| 33/35 [02:01<00:07,  3.78s/it]\u001b[A\n",
            " 97%|█████████▋| 34/35 [02:04<00:03,  3.56s/it]\u001b[A\n",
            "100%|██████████| 35/35 [02:05<00:00,  2.67s/it]\u001b[A\n",
            " 40%|████      | 2/5 [04:01<06:05, 121.74s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "Loading pretrained parameter \"ffn.7.weight\".\n",
            "Loading pretrained parameter \"ffn.7.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:08<05:05,  8.99s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:11<02:58,  5.41s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:14<02:14,  4.21s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:19<02:13,  4.30s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:21<01:50,  3.70s/it]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:25<01:44,  3.61s/it]\u001b[A\n",
            " 20%|██        | 7/35 [00:30<01:53,  4.05s/it]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:32<01:37,  3.61s/it]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:35<01:24,  3.24s/it]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:37<01:14,  2.99s/it]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:40<01:07,  2.81s/it]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:44<01:13,  3.20s/it]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:48<01:19,  3.59s/it]\u001b[A\n",
            " 40%|████      | 14/35 [00:51<01:08,  3.27s/it]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:54<01:08,  3.43s/it]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:58<01:04,  3.38s/it]\u001b[A\n",
            " 49%|████▊     | 17/35 [01:00<00:54,  3.06s/it]\u001b[A\n",
            " 51%|█████▏    | 18/35 [01:03<00:50,  2.96s/it]\u001b[A\n",
            " 54%|█████▍    | 19/35 [01:05<00:43,  2.73s/it]\u001b[A\n",
            " 57%|█████▋    | 20/35 [01:08<00:42,  2.81s/it]\u001b[A\n",
            " 60%|██████    | 21/35 [01:11<00:40,  2.89s/it]\u001b[A\n",
            " 63%|██████▎   | 22/35 [01:13<00:35,  2.74s/it]\u001b[A\n",
            " 66%|██████▌   | 23/35 [01:17<00:37,  3.10s/it]\u001b[A\n",
            " 69%|██████▊   | 24/35 [01:20<00:31,  2.83s/it]\u001b[A\n",
            " 71%|███████▏  | 25/35 [01:22<00:27,  2.78s/it]\u001b[A\n",
            " 74%|███████▍  | 26/35 [01:26<00:26,  2.96s/it]\u001b[A\n",
            " 77%|███████▋  | 27/35 [01:29<00:23,  2.95s/it]\u001b[A\n",
            " 80%|████████  | 28/35 [01:33<00:23,  3.32s/it]\u001b[A\n",
            " 83%|████████▎ | 29/35 [01:38<00:23,  3.88s/it]\u001b[A\n",
            " 86%|████████▌ | 30/35 [01:41<00:17,  3.57s/it]\u001b[A\n",
            " 89%|████████▊ | 31/35 [01:44<00:13,  3.39s/it]\u001b[A\n",
            " 91%|█████████▏| 32/35 [01:48<00:11,  3.74s/it]\u001b[A\n",
            " 94%|█████████▍| 33/35 [01:52<00:07,  3.75s/it]\u001b[A\n",
            " 97%|█████████▋| 34/35 [01:55<00:03,  3.54s/it]\u001b[A\n",
            "100%|██████████| 35/35 [01:56<00:00,  2.65s/it]\u001b[A\n",
            " 60%|██████    | 3/5 [05:58<03:59, 119.54s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "Loading pretrained parameter \"ffn.7.weight\".\n",
            "Loading pretrained parameter \"ffn.7.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:08<05:03,  8.92s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:11<02:57,  5.39s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:14<02:07,  4.00s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:18<02:09,  4.18s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:22<02:05,  4.18s/it]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:26<01:52,  3.90s/it]\u001b[A\n",
            " 20%|██        | 7/35 [00:30<01:50,  3.96s/it]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:33<01:37,  3.59s/it]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:36<01:28,  3.40s/it]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:38<01:16,  3.04s/it]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:40<01:07,  2.80s/it]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:43<01:04,  2.81s/it]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:48<01:17,  3.53s/it]\u001b[A\n",
            " 40%|████      | 14/35 [00:51<01:08,  3.26s/it]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:54<01:07,  3.39s/it]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:57<00:57,  3.03s/it]\u001b[A\n",
            " 49%|████▊     | 17/35 [01:00<00:55,  3.10s/it]\u001b[A\n",
            " 51%|█████▏    | 18/35 [01:03<00:52,  3.09s/it]\u001b[A\n",
            " 54%|█████▍    | 19/35 [01:05<00:44,  2.76s/it]\u001b[A\n",
            " 57%|█████▋    | 20/35 [01:07<00:40,  2.70s/it]\u001b[A\n",
            " 60%|██████    | 21/35 [01:10<00:36,  2.58s/it]\u001b[A\n",
            " 63%|██████▎   | 22/35 [01:13<00:34,  2.66s/it]\u001b[A\n",
            " 66%|██████▌   | 23/35 [01:17<00:38,  3.20s/it]\u001b[A\n",
            " 69%|██████▊   | 24/35 [01:19<00:32,  2.93s/it]\u001b[A\n",
            " 71%|███████▏  | 25/35 [01:21<00:26,  2.66s/it]\u001b[A\n",
            " 74%|███████▍  | 26/35 [01:24<00:24,  2.69s/it]\u001b[A\n",
            " 77%|███████▋  | 27/35 [01:28<00:24,  3.08s/it]\u001b[A\n",
            " 80%|████████  | 28/35 [01:36<00:32,  4.61s/it]\u001b[A\n",
            " 83%|████████▎ | 29/35 [01:43<00:30,  5.12s/it]\u001b[A\n",
            " 86%|████████▌ | 30/35 [01:45<00:22,  4.44s/it]\u001b[A\n",
            " 89%|████████▊ | 31/35 [01:48<00:15,  3.97s/it]\u001b[A\n",
            " 91%|█████████▏| 32/35 [01:54<00:13,  4.35s/it]\u001b[A\n",
            " 94%|█████████▍| 33/35 [01:57<00:08,  4.09s/it]\u001b[A\n",
            " 97%|█████████▋| 34/35 [02:00<00:03,  3.82s/it]\u001b[A\n",
            "100%|██████████| 35/35 [02:01<00:00,  2.86s/it]\u001b[A\n",
            " 80%|████████  | 4/5 [08:00<02:00, 120.54s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "Loading pretrained parameter \"ffn.7.weight\".\n",
            "Loading pretrained parameter \"ffn.7.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:12<07:09, 12.65s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:15<03:51,  7.02s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:18<02:48,  5.26s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:22<02:28,  4.80s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:25<02:00,  4.02s/it]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:28<01:46,  3.69s/it]\u001b[A\n",
            " 20%|██        | 7/35 [00:33<01:54,  4.08s/it]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:35<01:34,  3.49s/it]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:38<01:21,  3.13s/it]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:40<01:14,  2.97s/it]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:43<01:06,  2.77s/it]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:47<01:12,  3.14s/it]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:50<01:13,  3.34s/it]\u001b[A\n",
            " 40%|████      | 14/35 [00:53<01:05,  3.11s/it]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:56<01:01,  3.09s/it]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:59<01:00,  3.17s/it]\u001b[A\n",
            " 49%|████▊     | 17/35 [01:02<00:51,  2.87s/it]\u001b[A\n",
            " 51%|█████▏    | 18/35 [01:04<00:47,  2.78s/it]\u001b[A\n",
            " 54%|█████▍    | 19/35 [01:06<00:40,  2.52s/it]\u001b[A\n",
            " 57%|█████▋    | 20/35 [01:08<00:37,  2.51s/it]\u001b[A\n",
            " 60%|██████    | 21/35 [01:12<00:38,  2.73s/it]\u001b[A\n",
            " 63%|██████▎   | 22/35 [01:14<00:35,  2.73s/it]\u001b[A\n",
            " 66%|██████▌   | 23/35 [01:18<00:37,  3.12s/it]\u001b[A\n",
            " 69%|██████▊   | 24/35 [01:21<00:31,  2.85s/it]\u001b[A\n",
            " 71%|███████▏  | 25/35 [01:23<00:26,  2.65s/it]\u001b[A\n",
            " 74%|███████▍  | 26/35 [01:27<00:27,  3.04s/it]\u001b[A\n",
            " 77%|███████▋  | 27/35 [01:30<00:23,  3.00s/it]\u001b[A\n",
            " 80%|████████  | 28/35 [01:34<00:24,  3.45s/it]\u001b[A\n",
            " 83%|████████▎ | 29/35 [01:40<00:24,  4.01s/it]\u001b[A\n",
            " 86%|████████▌ | 30/35 [01:42<00:18,  3.68s/it]\u001b[A\n",
            " 89%|████████▊ | 31/35 [01:45<00:13,  3.45s/it]\u001b[A\n",
            " 91%|█████████▏| 32/35 [01:50<00:11,  3.81s/it]\u001b[A\n",
            " 94%|█████████▍| 33/35 [01:54<00:07,  3.85s/it]\u001b[A\n",
            " 97%|█████████▋| 34/35 [01:57<00:03,  3.60s/it]\u001b[A\n",
            "100%|██████████| 35/35 [01:58<00:00,  2.70s/it]\u001b[A\n",
            "100%|██████████| 5/5 [09:59<00:00, 119.87s/it]Saving predictions to /dev/null\n",
            "Elapsed time = 0:10:00\n",
            "\n",
            "                                                 smiles  ... peakwavs_max_pred\n",
            "0                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        375.545257\n",
            "1                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        390.993980\n",
            "2                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        397.488817\n",
            "3                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        400.081324\n",
            "4                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...        412.967337\n",
            "...                                                 ...  ...               ...\n",
            "1705           c1cc2c3ccc[n+]4cccc(c5ccc[n+](c1)c25)c34  ...        424.124035\n",
            "1706           c1cc2c3ccc[n+]4cccc(c5ccc[n+](c1)c25)c34  ...        428.538180\n",
            "1707  COc1cc(C)c(-c2cc(-c3c(C)cc(OC)cc3C)c3ccc4c(-c5...  ...        355.781207\n",
            "1708  N#Cc1c(N2CCCCC2)cc(-c2cccc3ccccc23)c2c1-c1cccc...  ...        358.098561\n",
            "1709        N#Cc1c(N2CCCC2)cc(-c2ccccc2)c2c1Cc1ccccc1-2  ...        380.867901\n",
            "\n",
            "[1710 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "%%py38\n",
        "# TDDFT Predictions\n",
        "arguments = [\n",
        "  '--test_path', test_file,\n",
        "  '--preds_path', 'test_tddft_preds.csv',\n",
        "  '--checkpoint_dir', 'models/lambda_max_abs_wb97xd3/chemprop/all_wb97xd3/production/fold_0',\n",
        "  '--number_of_molecules', '1',\n",
        "  #'--gpu', '0'\n",
        "]\n",
        "\n",
        "args = chemprop.args.PredictArgs().parse_args(arguments)\n",
        "_ = chemprop.train.make_predictions(args=args)\n",
        "\n",
        "# Convert Predictions to Features File\n",
        "!python models/tddft_to_features_file.py\n",
        "\n",
        "# Experimental Predictions\n",
        "arguments = [\n",
        "  '--test_path', test_file,\n",
        "  '--preds_path', '/dev/null',\n",
        "  '--checkpoint_dir', 'models/lambda_max_abs/chemprop_tddft/combined/production/fold_0',\n",
        "  '--number_of_molecules', '2',\n",
        "  '--features_path', 'features_test.csv'\n",
        "  #'--gpu', '0'\n",
        "]\n",
        "\n",
        "args = chemprop.args.PredictArgs().parse_args(arguments)\n",
        "preds = chemprop.train.make_predictions(args=args)\n",
        "\n",
        "preds = [x[0] for x in preds]\n",
        "df['peakwavs_max_pred'] = preds\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHGJvB47P8lI"
      },
      "source": [
        "## Predict experimental peak with model trained on combined training set (with ensemble variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBPuuTzeQCfW"
      },
      "source": [
        "**Equivalent to command line:**\n",
        "\n",
        "python uvvisml/predict.py --test_file uvvisml/data/splits/lambda_max_abs/deep4chem/group_by_smiles/smiles_target_test.csv --property absorption_peak_nm_expt --method chemprop --preds_file test_preds.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "t--C7XlAP8bD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c69ddfdd-e698-4955-b1b1-52c30bd783c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training args\n",
            "/usr/local/lib/python3.8/site-packages/chemprop/utils.py:472: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "Setting molecule featurization parameters to default.\n",
            "Loading data\n",
            "1710it [00:00, 145052.38it/s]\n",
            "100%|██████████| 1710/1710 [00:00<00:00, 7184.86it/s]\n",
            "/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Validating SMILES\n",
            "Test size = 1,710\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.8/site-packages/chemprop/utils.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
            "/usr/local/lib/python3.8/site-packages/chemprop/utils.py:417: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "Loading pretrained parameter \"ffn.7.weight\".\n",
            "Loading pretrained parameter \"ffn.7.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:06<03:50,  6.77s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:06<01:35,  2.89s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:07<00:53,  1.69s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:07<00:35,  1.13s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:07<00:23,  1.29it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:07<00:16,  1.77it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:08<00:13,  2.01it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:08<00:10,  2.66it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:08<00:08,  3.10it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:08<00:06,  3.75it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:08<00:05,  4.27it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:09<00:05,  4.04it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:09<00:04,  4.54it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:09<00:04,  4.74it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:09<00:05,  3.37it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:10<00:04,  4.13it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:10<00:03,  4.63it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:10<00:03,  4.77it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:10<00:03,  4.82it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:10<00:03,  4.20it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:11<00:02,  6.24it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:11<00:03,  3.79it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:11<00:01,  5.55it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:11<00:01,  6.91it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:12<00:01,  6.21it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:12<00:00,  6.71it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:12<00:00,  7.34it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:12<00:00,  7.75it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:12<00:00,  8.83it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:13<00:52, 13.12s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "Loading pretrained parameter \"ffn.7.weight\".\n",
            "Loading pretrained parameter \"ffn.7.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:06<03:33,  6.29s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:06<01:33,  2.85s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:06<00:53,  1.66s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:07<00:37,  1.21s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:07<00:25,  1.17it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:07<00:18,  1.54it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:08<00:13,  2.07it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:08<00:10,  2.52it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:08<00:08,  3.02it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:08<00:07,  3.45it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:08<00:06,  3.91it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:09<00:06,  3.63it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:09<00:05,  4.06it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:09<00:05,  3.88it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:10<00:05,  3.59it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:10<00:05,  3.69it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:10<00:05,  3.52it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:11<00:06,  2.59it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:11<00:05,  2.75it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:12<00:03,  4.04it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:12<00:02,  4.62it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:12<00:01,  6.37it/s]\u001b[A\n",
            " 74%|███████▍  | 26/35 [00:12<00:01,  6.22it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:12<00:01,  6.01it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:12<00:00,  6.43it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:13<00:00,  7.76it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:13<00:00,  8.14it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:13<00:00,  8.52it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:26<00:40, 13.49s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "Loading pretrained parameter \"ffn.7.weight\".\n",
            "Loading pretrained parameter \"ffn.7.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:06<03:41,  6.53s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:06<01:33,  2.84s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:06<00:52,  1.64s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:07<00:35,  1.16s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:07<00:25,  1.19it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:07<00:18,  1.57it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:08<00:14,  1.89it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:08<00:10,  2.54it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:08<00:08,  3.00it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:08<00:07,  3.54it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:09<00:06,  3.76it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:09<00:05,  4.09it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:09<00:04,  4.21it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:10<00:08,  2.50it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:10<00:06,  3.05it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:10<00:04,  3.64it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:10<00:04,  3.90it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:11<00:04,  3.74it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:11<00:05,  2.97it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:11<00:02,  4.69it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:12<00:02,  4.83it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:12<00:01,  6.92it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:12<00:00,  8.56it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:12<00:00,  7.70it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:12<00:00,  8.26it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:12<00:00,  8.50it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:13<00:00,  9.20it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:40<00:27, 13.50s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "Loading pretrained parameter \"ffn.7.weight\".\n",
            "Loading pretrained parameter \"ffn.7.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:08<04:46,  8.44s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:08<01:59,  3.62s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:08<01:05,  2.04s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:09<00:42,  1.36s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:09<00:27,  1.09it/s]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:09<00:19,  1.50it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:09<00:14,  1.88it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:10<00:09,  2.86it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:10<00:07,  3.30it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:10<00:06,  3.73it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:10<00:08,  2.87it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:11<00:06,  3.32it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:11<00:05,  4.07it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:11<00:05,  3.34it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:11<00:04,  3.81it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:11<00:03,  4.63it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:12<00:03,  4.37it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:12<00:04,  3.75it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:12<00:03,  3.98it/s]\u001b[A\n",
            " 63%|██████▎   | 22/35 [00:12<00:02,  6.10it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:13<00:01,  6.15it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:13<00:01,  8.26it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:13<00:00,  8.92it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:13<00:00,  7.91it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:13<00:00,  8.17it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:14<00:00,  7.91it/s]\u001b[A\n",
            " 94%|█████████▍| 33/35 [00:14<00:00,  7.82it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:14<00:00,  7.80it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:55<00:14, 14.01s/it]Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
            "Loading pretrained parameter \"ffn.1.weight\".\n",
            "Loading pretrained parameter \"ffn.1.bias\".\n",
            "Loading pretrained parameter \"ffn.4.weight\".\n",
            "Loading pretrained parameter \"ffn.4.bias\".\n",
            "Loading pretrained parameter \"ffn.7.weight\".\n",
            "Loading pretrained parameter \"ffn.7.bias\".\n",
            "\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|▎         | 1/35 [00:09<05:26,  9.59s/it]\u001b[A\n",
            "  6%|▌         | 2/35 [00:09<02:12,  4.02s/it]\u001b[A\n",
            "  9%|▊         | 3/35 [00:09<01:12,  2.27s/it]\u001b[A\n",
            " 11%|█▏        | 4/35 [00:10<00:55,  1.78s/it]\u001b[A\n",
            " 14%|█▍        | 5/35 [00:11<00:36,  1.21s/it]\u001b[A\n",
            " 17%|█▋        | 6/35 [00:11<00:25,  1.15it/s]\u001b[A\n",
            " 20%|██        | 7/35 [00:11<00:18,  1.48it/s]\u001b[A\n",
            " 23%|██▎       | 8/35 [00:11<00:14,  1.91it/s]\u001b[A\n",
            " 26%|██▌       | 9/35 [00:11<00:10,  2.48it/s]\u001b[A\n",
            " 29%|██▊       | 10/35 [00:12<00:07,  3.18it/s]\u001b[A\n",
            " 31%|███▏      | 11/35 [00:12<00:06,  3.96it/s]\u001b[A\n",
            " 34%|███▍      | 12/35 [00:12<00:06,  3.57it/s]\u001b[A\n",
            " 37%|███▋      | 13/35 [00:12<00:06,  3.20it/s]\u001b[A\n",
            " 40%|████      | 14/35 [00:13<00:05,  4.00it/s]\u001b[A\n",
            " 43%|████▎     | 15/35 [00:13<00:05,  3.67it/s]\u001b[A\n",
            " 46%|████▌     | 16/35 [00:13<00:04,  4.21it/s]\u001b[A\n",
            " 49%|████▊     | 17/35 [00:13<00:03,  4.64it/s]\u001b[A\n",
            " 51%|█████▏    | 18/35 [00:13<00:03,  4.88it/s]\u001b[A\n",
            " 54%|█████▍    | 19/35 [00:14<00:03,  4.36it/s]\u001b[A\n",
            " 57%|█████▋    | 20/35 [00:14<00:03,  4.73it/s]\u001b[A\n",
            " 60%|██████    | 21/35 [00:14<00:02,  5.45it/s]\u001b[A\n",
            " 66%|██████▌   | 23/35 [00:14<00:01,  6.25it/s]\u001b[A\n",
            " 71%|███████▏  | 25/35 [00:14<00:01,  8.17it/s]\u001b[A\n",
            " 77%|███████▋  | 27/35 [00:15<00:00,  8.54it/s]\u001b[A\n",
            " 80%|████████  | 28/35 [00:15<00:00,  7.48it/s]\u001b[A\n",
            " 83%|████████▎ | 29/35 [00:15<00:00,  6.37it/s]\u001b[A\n",
            " 86%|████████▌ | 30/35 [00:15<00:00,  5.93it/s]\u001b[A\n",
            " 89%|████████▊ | 31/35 [00:16<00:01,  3.77it/s]\u001b[A\n",
            " 91%|█████████▏| 32/35 [00:16<00:00,  4.49it/s]\u001b[A\n",
            " 97%|█████████▋| 34/35 [00:16<00:00,  6.26it/s]\u001b[A\n",
            "100%|██████████| 5/5 [01:12<00:00, 14.41s/it]Saving predictions to test_preds.csv\n",
            "Elapsed time = 0:01:13\n",
            "\n",
            "                                                 smiles  ... peakwavs_max_ensemble_uncal_var\n",
            "0                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...                        5.984684\n",
            "1                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...                        3.289096\n",
            "2                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...                        6.647632\n",
            "3                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...                        7.351677\n",
            "4                   CCN(CC)c1ccc2c(C(F)(F)F)cc(=O)oc2c1  ...                        5.109207\n",
            "...                                                 ...  ...                             ...\n",
            "1705           c1cc2c3ccc[n+]4cccc(c5ccc[n+](c1)c25)c34  ...                        8.989382\n",
            "1706           c1cc2c3ccc[n+]4cccc(c5ccc[n+](c1)c25)c34  ...                        9.853369\n",
            "1707  COc1cc(C)c(-c2cc(-c3c(C)cc(OC)cc3C)c3ccc4c(-c5...  ...                      128.172913\n",
            "1708  N#Cc1c(N2CCCCC2)cc(-c2cccc3ccccc23)c2c1-c1cccc...  ...                        2.114018\n",
            "1709        N#Cc1c(N2CCCC2)cc(-c2ccccc2)c2c1Cc1ccccc1-2  ...                       11.947813\n",
            "\n",
            "[1710 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "%%py38\n",
        "arguments = [\n",
        "  '--test_path', test_file,\n",
        "  '--preds_path', 'test_preds.csv',\n",
        "  '--checkpoint_dir', 'models/lambda_max_abs/chemprop/combined/production/fold_0',\n",
        "  '--number_of_molecules', '2',\n",
        "  '--ensemble_variance',\n",
        "  #'--gpu', '0'\n",
        "]\n",
        "\n",
        "args = chemprop.args.PredictArgs().parse_args(arguments)\n",
        "_ = chemprop.train.make_predictions(args=args)\n",
        "\n",
        "df = pd.read_csv('test_preds.csv')\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8",
      "name": "py38"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
